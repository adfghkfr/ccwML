{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Read data\n",
    "df = pd.read_excel(\"/Users/guoyixuan/Documents/pythoncode/ccwmachine/wine.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original data\n",
    "X = np.array(df.iloc[:, :-1])  #排除最後一欄分類標籤 N x p \n",
    "#178 13\n",
    "y = np.array(df.iloc[:, -1])\n",
    "#178 1\n",
    "\n",
    "# Split data into training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30)\n",
    "#124 13  #54 13  #124 1  #54 1\n",
    "\n",
    "# Standardize data\n",
    "scaler = StandardScaler()\n",
    "X_train_ = scaler.fit_transform(X_train)\n",
    "X_test_ = scaler.fit_transform(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以訓練資料估計分類器模型的參數，待估計完成後，再以測試資料檢驗訓練結果\n",
    "(注意: 訓練資料與測試資料必須分開標準化，而非標準化後再分成訓練與測試資料)\n",
    "\n",
    "假設表現依然不被接受，還 可以調整資料，譬如進行標準化、改為特徵資料或改變特徵的選取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.36228D+02    |proj g|=  1.32080D+04\n",
      "\n",
      "At iterate   50    f=  2.12385D+01    |proj g|=  6.60261D+02\n",
      "\n",
      "At iterate  100    f=  1.56193D+01    |proj g|=  2.12991D+02\n",
      "\n",
      "At iterate  150    f=  1.42355D+01    |proj g|=  1.69649D+02\n",
      "\n",
      "At iterate  200    f=  1.38992D+01    |proj g|=  6.53051D+00\n",
      "\n",
      "At iterate  250    f=  1.37612D+01    |proj g|=  3.72972D+01\n",
      "\n",
      "At iterate  300    f=  1.36762D+01    |proj g|=  4.72041D+00\n",
      "\n",
      "At iterate  350    f=  1.34866D+01    |proj g|=  7.12088D+01\n",
      "\n",
      "At iterate  400    f=  1.33817D+01    |proj g|=  3.08997D+01\n",
      "\n",
      "At iterate  450    f=  1.33012D+01    |proj g|=  5.73410D+01\n",
      "\n",
      "At iterate  500    f=  1.32621D+01    |proj g|=  3.76115D+00\n",
      "\n",
      "At iterate  550    f=  1.32563D+01    |proj g|=  5.92554D+00\n",
      "\n",
      "At iterate  600    f=  1.32458D+01    |proj g|=  2.33886D+00\n",
      "\n",
      "At iterate  650    f=  1.32112D+01    |proj g|=  6.19039D+01\n",
      "\n",
      "At iterate  700    f=  1.31597D+01    |proj g|=  3.56662D+00\n",
      "\n",
      "At iterate  750    f=  1.31099D+01    |proj g|=  1.49216D+01\n",
      "\n",
      "At iterate  800    f=  1.31018D+01    |proj g|=  1.59707D+00\n",
      "\n",
      "At iterate  850    f=  1.30865D+01    |proj g|=  1.25253D+00\n",
      "\n",
      "At iterate  900    f=  1.30555D+01    |proj g|=  7.79844D-01\n",
      "\n",
      "At iterate  950    f=  1.30190D+01    |proj g|=  5.93394D+01\n",
      "\n",
      "At iterate 1000    f=  1.29789D+01    |proj g|=  1.78504D+01\n",
      "\n",
      "At iterate 1050    f=  1.29257D+01    |proj g|=  1.23369D+01\n",
      "\n",
      "At iterate 1100    f=  1.29040D+01    |proj g|=  1.68656D+01\n",
      "\n",
      "At iterate 1150    f=  1.27790D+01    |proj g|=  8.67937D+00\n",
      "\n",
      "At iterate 1200    f=  1.25986D+01    |proj g|=  1.86175D+01\n",
      "\n",
      "At iterate 1250    f=  1.25811D+01    |proj g|=  2.25670D+00\n",
      "\n",
      "At iterate 1300    f=  1.25661D+01    |proj g|=  2.27114D+00\n",
      "\n",
      "At iterate 1350    f=  1.25349D+01    |proj g|=  5.39736D+01\n",
      "\n",
      "At iterate 1400    f=  1.22907D+01    |proj g|=  6.71590D+00\n",
      "\n",
      "At iterate 1450    f=  1.21181D+01    |proj g|=  5.79292D+01\n",
      "\n",
      "At iterate 1500    f=  1.18432D+01    |proj g|=  2.34576D+02\n",
      "\n",
      "At iterate 1550    f=  1.15053D+01    |proj g|=  1.49591D+01\n",
      "\n",
      "At iterate 1600    f=  1.07620D+01    |proj g|=  1.78629D+02\n",
      "\n",
      "At iterate 1650    f=  1.06355D+01    |proj g|=  2.26591D+01\n",
      "\n",
      "At iterate 1700    f=  1.01321D+01    |proj g|=  1.66614D+01\n",
      "\n",
      "At iterate 1750    f=  9.84915D+00    |proj g|=  7.19959D+01\n",
      "\n",
      "At iterate 1800    f=  9.67972D+00    |proj g|=  1.53731D+02\n",
      "\n",
      "At iterate 1850    f=  9.41548D+00    |proj g|=  2.15143D+00\n",
      "\n",
      "At iterate 1900    f=  9.36383D+00    |proj g|=  1.01422D+00\n",
      "\n",
      "At iterate 1950    f=  9.34627D+00    |proj g|=  1.33846D+01\n",
      "\n",
      "At iterate 2000    f=  9.33662D+00    |proj g|=  6.22554D+00\n",
      "\n",
      "At iterate 2050    f=  9.29046D+00    |proj g|=  1.88777D+00\n",
      "\n",
      "At iterate 2100    f=  9.25032D+00    |proj g|=  1.60793D+00\n",
      "\n",
      "At iterate 2150    f=  9.24810D+00    |proj g|=  2.18514D-01\n",
      "\n",
      "At iterate 2200    f=  9.24771D+00    |proj g|=  8.99708D-01\n",
      "\n",
      "At iterate 2250    f=  9.24542D+00    |proj g|=  2.70198D+00\n",
      "\n",
      "At iterate 2300    f=  9.24447D+00    |proj g|=  6.16870D-01\n",
      "\n",
      "At iterate 2350    f=  9.24358D+00    |proj g|=  1.35168D+00\n",
      "\n",
      "At iterate 2400    f=  9.24272D+00    |proj g|=  9.88858D-01\n",
      "\n",
      "At iterate 2450    f=  9.24236D+00    |proj g|=  8.08922D-01\n",
      "\n",
      "At iterate 2500    f=  9.24215D+00    |proj g|=  2.55941D+00\n",
      "\n",
      "At iterate 2550    f=  9.24197D+00    |proj g|=  1.70867D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42   2592   2921      1     0     0   2.780D-02   9.242D+00\n",
      "  F =   9.2418033888221771     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        20\n",
      "           2       0.95      1.00      0.97        18\n",
      "           3       1.00      0.94      0.97        16\n",
      "\n",
      "    accuracy                           0.98        54\n",
      "   macro avg       0.98      0.98      0.98        54\n",
      "weighted avg       0.98      0.98      0.98        54\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "###########利用原始資料來進行羅吉斯回歸分析###########\n",
    "opts = dict(tol = 1e-6, max_iter = int(1e6), verbose = 1)\n",
    "solver = \"lbfgs\" # ’lbfgs’ is the default\n",
    "\n",
    "clf_original = LogisticRegression(solver = solver, **opts) #logistic regression model\n",
    "clf_original.fit(X_train, y_train) #fit the model according to the given training data\n",
    "\n",
    "# 利用模型預測測試資料\n",
    "y_pred = clf_original.predict(X_test) #predict class labels for samples in X_test\n",
    "\n",
    "# 測 試 資 料 之 準 確 率 回 報\n",
    "#print(f\"{accuracy_score(y_test, y_pred):.2%}\\n\")\n",
    "#print(f\"{clf_original.score(X_test_, y_test):.2%}\\n\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.36228D+02    |proj g|=  4.74684D+01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     25     27      1     0     0   3.147D-05   9.919D+00\n",
      "  F =   9.9189658417550586     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        20\n",
      "           2       1.00      1.00      1.00        18\n",
      "           3       1.00      1.00      1.00        16\n",
      "\n",
      "    accuracy                           1.00        54\n",
      "   macro avg       1.00      1.00      1.00        54\n",
      "weighted avg       1.00      1.00      1.00        54\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "##########使用標準化後的資料來進行羅吉斯回歸分析##########\n",
    "\n",
    "# 參數估計的演算法所需的調整細節放在 opts中\n",
    "opts = dict(tol = 1e-6, max_iter = int(1e6), verbose = 1)\n",
    "solver = \"lbfgs\" # ’lbfgs’ is the default\n",
    "# solver = ’liblinear’\n",
    "# solver = ’newton−cg’\n",
    "clf_original = LogisticRegression(solver = solver, **opts) #logistic regression model\n",
    "clf_original.fit(X_train_, y_train) #fit the model according to the given training data\n",
    "\n",
    "# 利用模型預測測試資料\n",
    "y_pred = clf_original.predict(X_test_) #predict class labels for samples in X_test\n",
    "\n",
    "# 測 試 資 料 之 準 確 率 回 報\n",
    "#print(f\"{accuracy_score(y_test, y_pred):.2%}\\n\")\n",
    "#print(f\"{clf_original.score(X_test_, y_test):.2%}\\n\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用 LogisticRegressionCV 建立羅吉斯回歸模型，與使用 LogisticRegression 的模型之結果比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.08763D+02    |proj g|=  3.74398D+01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     15     27      1     0     0   2.355D-03   1.067D+02\n",
      "  F =   106.68708683306929     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.06628D+02    |proj g|=  2.62307D+01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42      2      4      1     0     0   3.547D-03   1.065D+02\n",
      "  F =   106.49068936545376     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.06295D+02    |proj g|=  2.61677D+01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42      6     11      1     0     0   1.299D-02   1.058D+02\n",
      "  F =   105.83840333390958     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.05192D+02    |proj g|=  2.59545D+01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42      3      5      1     0     0   1.198D-02   1.037D+02\n",
      "  F =   103.72945734610941     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.01682D+02    |proj g|=  2.52686D+01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     13     17      1     0     0   7.498D-03   9.747D+01\n",
      "  F =   97.471974053836703     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  9.17818D+01    |proj g|=  2.31754D+01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     14     17      1     0     0   6.910D-05   8.280D+01\n",
      "  F =   82.797353752673857     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  7.14765D+01    |proj g|=  1.81945D+01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     13     16      1     0     0   6.621D-05   6.040D+01\n",
      "  F =   60.404898128344200     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  4.67265D+01    |proj g|=  1.11799D+01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     13     16      1     0     0   1.752D-04   3.846D+01\n",
      "  F =   38.455402849811676     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.72041D+01    |proj g|=  5.58637D+00\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     14     15      1     0     0   1.369D-04   2.227D+01\n",
      "  F =   22.270658834088671     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.47389D+01    |proj g|=  2.48014D+00\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     18     19      1     0     0   8.764D-05   1.203D+01\n",
      "  F =   12.032603317858499     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  7.52507D+00    |proj g|=  1.10336D+00\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     21     25      1     0     0   2.987D-05   6.121D+00\n",
      "  F =   6.1213140739076497     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  3.61942D+00    |proj g|=  4.67033D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     26     31      1     0     0   2.171D-05   2.936D+00\n",
      "  F =   2.9363377631341994     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.63647D+00    |proj g|=  1.88853D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     29     33      1     0     0   5.547D-06   1.330D+00\n",
      "  F =   1.3301541779003994     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.98840D-01    |proj g|=  7.32653D-02\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     33     38      1     0     0   8.239D-06   5.722D-01\n",
      "  F =  0.57221109516241375     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.84740D-01    |proj g|=  2.73990D-02\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     33     39      1     0     0   7.325D-06   2.356D-01\n",
      "  F =  0.23557026575437551     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.11750D-01    |proj g|=  9.93309D-03\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     33     39      1     0     0   1.623D-06   9.350D-02\n",
      "  F =   9.3500164502463526E-002\n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  4.25557D-02    |proj g|=  3.51020D-03\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     33     38      1     0     0   7.809D-07   3.600D-02\n",
      "  F =   3.6003452023494711E-002\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.58110D-02    |proj g|=  1.21473D-03\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     29     34      1     0     0   9.147D-07   1.352D-02\n",
      "  F =   1.3517867444289348E-002\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  5.75491D-03    |proj g|=  4.13561D-04\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     24     28      1     0     0   6.155D-07   4.969D-03\n",
      "  F =   4.9688878601939950E-003\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.05935D-03    |proj g|=  1.38801D-04\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     11     13      1     0     0   7.005D-07   1.794D-03\n",
      "  F =   1.7939951490648066E-003\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.08763D+02    |proj g|=  3.78009D+01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     15     27      1     0     0   2.602D-03   1.067D+02\n",
      "  F =   106.68121703467246     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.06618D+02    |proj g|=  2.65180D+01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42      2      4      1     0     0   7.265D-04   1.065D+02\n",
      "  F =   106.47104260854977     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.06261D+02    |proj g|=  2.64491D+01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42      3      6      1     0     0   4.018D-03   1.058D+02\n",
      "  F =   105.77323423325782     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.05082D+02    |proj g|=  2.62203D+01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42      9     12      1     0     0   3.005D-03   1.035D+02\n",
      "  F =   103.51970876650020     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.01334D+02    |proj g|=  2.54741D+01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     14     17      1     0     0   1.045D-03   9.686D+01\n",
      "  F =   96.860822773875029     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  9.08275D+01    |proj g|=  2.32110D+01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     14     17      1     0     0   4.886D-05   8.145D+01\n",
      "  F =   81.451945963206796     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.96892D+01    |proj g|=  1.79314D+01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     13     15      1     0     0   7.738D-05   5.855D+01\n",
      "  F =   58.554662902055071     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  4.47796D+01    |proj g|=  1.09652D+01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     12     14      1     0     0   2.063D-04   3.674D+01\n",
      "  F =   36.743213229693488     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.57091D+01    |proj g|=  5.43281D+00\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     14     15      1     0     0   1.004D-04   2.103D+01\n",
      "  F =   21.031709213397374     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.37977D+01    |proj g|=  2.40398D+00\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     18     19      1     0     0   5.194D-05   1.127D+01\n",
      "  F =   11.265322485778150     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.99346D+00    |proj g|=  1.02776D+00\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     22     23      1     0     0   4.326D-05   5.683D+00\n",
      "  F =   5.6831268691788850     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  3.33016D+00    |proj g|=  4.23273D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     24     26      1     0     0   3.498D-05   2.700D+00\n",
      "  F =   2.6998048870725206     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.48919D+00    |proj g|=  1.65494D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     25     29      1     0     0   1.615D-05   1.212D+00\n",
      "  F =   1.2116499871836455     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.30573D-01    |proj g|=  6.19391D-02\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     27     30      1     0     0   2.448D-05   5.173D-01\n",
      "  F =  0.51727660408441212     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.55400D-01    |proj g|=  2.25247D-02\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     28     31      1     0     0   5.746D-06   2.117D-01\n",
      "  F =  0.21170053572564440     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  9.97903D-02    |proj g|=  8.06946D-03\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     27     30      1     0     0   8.611D-06   8.364D-02\n",
      "  F =   8.3638538856062233E-002\n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  3.78684D-02    |proj g|=  2.83577D-03\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     28     32      1     0     0   1.873D-06   3.209D-02\n",
      "  F =   3.2086726923492834E-002\n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.40277D-02    |proj g|=  9.77521D-04\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     27     30      1     0     0   1.083D-06   1.201D-02\n",
      "  F =   1.2010600787953523E-002\n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  5.09466D-03    |proj g|=  3.31643D-04\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     23     27      1     0     0   9.108D-07   4.404D-03\n",
      "  F =   4.4036559140332070E-003\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.81956D-03    |proj g|=  1.11793D-04\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     18     20      1     0     0   9.054D-07   1.587D-03\n",
      "  F =   1.5865346839397507E-003\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.08763D+02    |proj g|=  3.88197D+01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     13     24      1     0     0   1.687D-02   1.067D+02\n",
      "  F =   106.68402588960964     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.06623D+02    |proj g|=  2.72521D+01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42      2      4      1     0     0   2.721D-03   1.065D+02\n",
      "  F =   106.48045554444252     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.06277D+02    |proj g|=  2.71864D+01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42      6     11      1     0     0   2.642D-02   1.058D+02\n",
      "  F =   105.80458574492768     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.05135D+02    |proj g|=  2.69496D+01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42      9     13      1     0     0   1.845D-03   1.036D+02\n",
      "  F =   103.62196016898170     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.01505D+02    |proj g|=  2.61936D+01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     10     13      1     0     0   7.302D-03   9.717D+01\n",
      "  F =   97.170785009907476     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  9.13231D+01    |proj g|=  2.38881D+01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     13     16      1     0     0   4.805D-04   8.221D+01\n",
      "  F =   82.205093927319226     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  7.07520D+01    |proj g|=  1.84764D+01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     13     16      1     0     0   1.586D-04   5.982D+01\n",
      "  F =   59.819190274132168     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  4.62995D+01    |proj g|=  1.11024D+01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     13     16      1     0     0   1.173D-04   3.832D+01\n",
      "  F =   38.322129451573772     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.73848D+01    |proj g|=  5.41355D+00\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     16     17      1     0     0   7.652D-05   2.260D+01\n",
      "  F =   22.602793177937624     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.52640D+01    |proj g|=  2.46812D+00\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     17     18      1     0     0   1.416D-04   1.253D+01\n",
      "  F =   12.525922373033907     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  8.01885D+00    |proj g|=  1.13083D+00\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     23     25      1     0     0   5.613D-05   6.515D+00\n",
      "  F =   6.5152410093497561     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  3.91948D+00    |proj g|=  5.01196D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     25     30      1     0     0   4.656D-05   3.166D+00\n",
      "  F =   3.1663824253514781     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.78089D+00    |proj g|=  2.10980D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     32     34      1     0     0   8.644D-06   1.443D+00\n",
      "  F =   1.4429952234570360     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  7.61271D-01    |proj g|=  8.38794D-02\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     36     39      1     0     0   5.542D-06   6.223D-01\n",
      "  F =  0.62225534601902355     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  3.10134D-01    |proj g|=  3.17579D-02\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     38     41      1     0     0   3.107D-06   2.564D-01\n",
      "  F =  0.25636035746949654     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.21647D-01    |proj g|=  1.15829D-02\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     37     41      1     0     0   1.972D-06   1.017D-01\n",
      "  F =  0.10174343153772079     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  4.62879D-02    |proj g|=  4.10534D-03\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     37     43      1     0     0   2.121D-06   3.916D-02\n",
      "  F =   3.9159237280712994E-002\n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.71833D-02    |proj g|=  1.42404D-03\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     35     39      1     0     0   1.095D-06   1.469D-02\n",
      "  F =   1.4693513506207533E-002\n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.25009D-03    |proj g|=  4.85430D-04\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     33     36      1     0     0   5.999D-07   5.397D-03\n",
      "  F =   5.3974247983125190E-003\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.23488D-03    |proj g|=  1.62795D-04\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     26     28      1     0     0   9.526D-07   1.947D-03\n",
      "  F =   1.9474318575272149E-003\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.08763D+02    |proj g|=  3.72312D+01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     13     24      1     0     0   1.925D-03   1.062D+02\n",
      "  F =   106.17429137659417     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.06114D+02    |proj g|=  2.61362D+01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42      2      4      1     0     0   2.509D-03   1.060D+02\n",
      "  F =   105.97338246279116     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.05773D+02    |proj g|=  2.60668D+01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42      6     11      1     0     0   1.717D-02   1.053D+02\n",
      "  F =   105.30634754513073     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.04645D+02    |proj g|=  2.58297D+01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42      9     11      1     0     0   5.831D-03   1.032D+02\n",
      "  F =   103.15222414964825     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.01063D+02    |proj g|=  2.50789D+01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     14     18      1     0     0   7.729D-04   9.678D+01\n",
      "  F =   96.784569484461812     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  9.10119D+01    |proj g|=  2.28038D+01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     13     16      1     0     0   4.817D-04   8.201D+01\n",
      "  F =   82.005074931257866     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  7.06891D+01    |proj g|=  1.76972D+01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     13     15      1     0     0   7.613D-05   5.986D+01\n",
      "  F =   59.859059241696961     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  4.64602D+01    |proj g|=  1.07839D+01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     12     14      1     0     0   2.214D-04   3.849D+01\n",
      "  F =   38.485091722810139     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.75629D+01    |proj g|=  5.39105D+00\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     16     18      1     0     0   1.519D-04   2.270D+01\n",
      "  F =   22.704653344900287     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.52919D+01    |proj g|=  2.39796D+00\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     18     20      1     0     0   6.230D-05   1.249D+01\n",
      "  F =   12.492217466679691     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  7.91872D+00    |proj g|=  1.06052D+00\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     22     25      1     0     0   4.240D-05   6.418D+00\n",
      "  F =   6.4176308169156417     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  3.81351D+00    |proj g|=  4.50444D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     27     29      1     0     0   1.994D-05   3.084D+00\n",
      "  F =   3.0841950762571653     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.71811D+00    |proj g|=  1.83337D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     32     37      1     0     0   1.008D-05   1.395D+00\n",
      "  F =   1.3954733049689501     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  7.31964D-01    |proj g|=  7.16624D-02\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     36     42      1     0     0   6.106D-06   5.996D-01\n",
      "  F =  0.59955051275130966     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.98014D-01    |proj g|=  2.69763D-02\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     36     42      1     0     0   5.803D-06   2.467D-01\n",
      "  F =  0.24667511328755801     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.16977D-01    |proj g|=  9.82539D-03\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     37     42      1     0     0   3.341D-06   9.791D-02\n",
      "  F =   9.7907650456825715E-002\n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  4.45724D-02    |proj g|=  3.48450D-03\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     36     41      1     0     0   1.573D-06   3.772D-02\n",
      "  F =   3.7716319735503295E-002\n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.65745D-02    |proj g|=  1.20825D-03\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     35     38      1     0     0   1.055D-06   1.417D-02\n",
      "  F =   1.4170459625582620E-002\n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.03913D-03    |proj g|=  4.12176D-04\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     29     32      1     0     0   7.342D-07   5.213D-03\n",
      "  F =   5.2129745291109439E-003\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.16380D-03    |proj g|=  1.38790D-04\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     23     26      1     0     0   9.613D-07   1.884D-03\n",
      "  F =   1.8836937113238048E-003\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.09861D+02    |proj g|=  3.85819D+01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     17     29      1     0     0   1.264D-04   1.073D+02\n",
      "  F =   107.32487092234621     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.07265D+02    |proj g|=  2.70707D+01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42      2      4      1     0     0   4.687D-04   1.071D+02\n",
      "  F =   107.12481409002662     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.06925D+02    |proj g|=  2.69964D+01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42      3      5      1     0     0   9.905D-04   1.065D+02\n",
      "  F =   106.46039138551195     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.05802D+02    |proj g|=  2.67490D+01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42      3      5      1     0     0   1.574D-02   1.043D+02\n",
      "  F =   104.31236617046721     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.02227D+02    |proj g|=  2.59426D+01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     12     15      1     0     0   5.580D-03   9.794D+01\n",
      "  F =   97.940584770096024     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  9.21489D+01    |proj g|=  2.35211D+01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     14     18      1     0     0   1.868D-04   8.302D+01\n",
      "  F =   83.021908290998198     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  7.15350D+01    |proj g|=  1.83505D+01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     13     15      1     0     0   3.152D-05   6.041D+01\n",
      "  F =   60.408094248896390     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  4.66688D+01    |proj g|=  1.12521D+01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     12     14      1     0     0   9.549D-05   3.849D+01\n",
      "  F =   38.486338283217329     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.73104D+01    |proj g|=  5.61365D+00\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     15     16      1     0     0   1.074D-04   2.245D+01\n",
      "  F =   22.452256370687529     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.49997D+01    |proj g|=  2.48297D+00\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     15     16      1     0     0   1.757D-04   1.229D+01\n",
      "  F =   12.294648914629814     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  7.79680D+00    |proj g|=  1.03030D+00\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     21     24      1     0     0   2.584D-05   6.347D+00\n",
      "  F =   6.3467303523965652     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  3.80117D+00    |proj g|=  4.32227D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     24     28      1     0     0   1.346D-05   3.078D+00\n",
      "  F =   3.0781937970880295     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.73150D+00    |proj g|=  1.78161D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     27     33      1     0     0   1.258D-05   1.405D+00\n",
      "  F =   1.4047948550861113     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  7.43040D-01    |proj g|=  7.01260D-02\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     29     35      1     0     0   6.775D-06   6.075D-01\n",
      "  F =  0.60746906350252472     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  3.03829D-01    |proj g|=  2.65987D-02\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     33     39      1     0     0   2.594D-06   2.510D-01\n",
      "  F =  0.25102803142488139     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.19550D-01    |proj g|=  9.75014D-03\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     34     39      1     0     0   5.213D-06   9.991D-02\n",
      "  F =   9.9910684729503832E-002\n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  4.56083D-02    |proj g|=  3.47420D-03\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     33     40      1     0     0   8.113D-07   3.855D-02\n",
      "  F =   3.8550959513107670E-002\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.69686D-02    |proj g|=  1.20952D-03\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     31     35      1     0     0   7.980D-07   1.450D-02\n",
      "  F =   1.4496969868983573E-002\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.18383D-03    |proj g|=  4.13929D-04\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     24     28      1     0     0   8.915D-07   5.335D-03\n",
      "  F =   5.3352899366834721E-003\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           42     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.21407D-03    |proj g|=  1.39850D-04\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   42     12     15      1     0     0   9.933D-07   1.928D-03\n",
      "  F =   1.9283021503360867E-003\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        20\n",
      "           2       0.95      1.00      0.97        18\n",
      "           3       1.00      0.94      0.97        16\n",
      "\n",
      "    accuracy                           0.98        54\n",
      "   macro avg       0.98      0.98      0.98        54\n",
      "weighted avg       0.98      0.98      0.98        54\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "opts = dict(tol = 1e-6, max_iter = int(1e6), verbose = 1)\n",
    "solver = \"lbfgs\" \n",
    "Cs = np.logspace(-5, 5, 20)\n",
    "clf_originalCV = LogisticRegressionCV(solver = solver, Cs = Cs, **opts)\n",
    "clf_originalCV.fit(X_train_, y_train)\n",
    "\n",
    "y_pred = clf_originalCV.predict(X_test_)\n",
    "#print(f\"{accuracy_score(y_test, y_pred):.2%}\\n\")\n",
    "#print(f\"{clf_originalCV.score(X_test_, y_test):.2%}\\n\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USE PCA TO TRAIN THE MODEL\n",
    "利用標準化後之資料的主成分訓練資料建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           18     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.36228D+02    |proj g|=  8.90757D+01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        20\n",
      "           2       1.00      1.00      1.00        18\n",
      "           3       1.00      1.00      1.00        16\n",
      "\n",
      "    accuracy                           1.00        54\n",
      "   macro avg       1.00      1.00      1.00        54\n",
      "weighted avg       1.00      1.00      1.00        54\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   18     22     25      1     0     0   7.677D-05   1.259D+01\n",
      "  F =   12.593250015817247     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 5).fit(X_train_)\n",
    "Z_train = pca.transform(X_train_)\n",
    "Z_test = pca.transform(X_test_)\n",
    "\n",
    "opts = dict(tol = 1e-6, max_iter = int(1e6), verbose=1)\n",
    "solver = \"lbfgs\" \n",
    "clf_PCA = LogisticRegression(solver = solver, **opts)\n",
    "clf_PCA.fit(Z_train, y_train)\n",
    "y_pred = clf_PCA.predict(Z_test)\n",
    "\n",
    "#print(f\"{accuracy_score(y_test, y_pred):.2%}\\n\")\n",
    "#print(f\"{clf_PCA.score(Z_test, y_test):.2%}\\n\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUPPORT VECTOR MACHINE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "針對 K 個類別內的每兩個類別製作一個分類器，總共有 K(K−1)/2 個分類器，當面對一個新資料 xnew 的分類時，讓 K(K−1)/2 個分類器都做分類，再計算 xnew 被分到哪一個類別的次數最多，以此類別作為最終分類的結果。\n",
    "\n",
    "sk-learn 套件提供了幾個 SVM 的學習方式，如 SVC, LinearSVC, NuSVC， 5其中以 SVC 較常被使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        20\n",
      "           2       0.95      1.00      0.97        18\n",
      "           3       1.00      0.94      0.97        16\n",
      "\n",
      "    accuracy                           0.98        54\n",
      "   macro avg       0.98      0.98      0.98        54\n",
      "weighted avg       0.98      0.98      0.98        54\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guoyixuan/Documents/pythoncode/ccwML/lib/python3.9/site-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=1000000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC, LinearSVC\n",
    "C = 1 # SVM regularization parameter\n",
    "opts = dict(C = C, tol = 1e-6, max_iter = int(1e6))\n",
    "# opts = dict(C = C, decision_function_shape = ’ovo’, tol = 1e−6, max_iter = int(1e6))\n",
    "\n",
    "# clf_svm = SVC(kernel=\"linear\", **opts)\n",
    "# clf_svm = SVC(kernel=”rbf”, gamma=0.2, **opts)\n",
    "clf_svm = SVC(kernel=\"poly\", degree=3, gamma=\"auto\", **opts)\n",
    "# clf_svm = LinearSVC(**opts) # one vs the rest\n",
    "clf_svm.fit(X_train, y_train)\n",
    "predictions = clf_svm.predict(X_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.26622433\n",
      "Iteration 2, loss = 1.12231089\n",
      "Iteration 3, loss = 1.03305764\n",
      "Iteration 4, loss = 0.99729766\n",
      "Iteration 5, loss = 0.99390987\n",
      "Iteration 6, loss = 0.99903452\n",
      "Iteration 7, loss = 0.99462699\n",
      "Iteration 8, loss = 0.97786358\n",
      "Iteration 9, loss = 0.95181898\n",
      "Iteration 10, loss = 0.92193034\n",
      "Iteration 11, loss = 0.89599072\n",
      "Iteration 12, loss = 0.87720255\n",
      "Iteration 13, loss = 0.86359022\n",
      "Iteration 14, loss = 0.85387168\n",
      "Iteration 15, loss = 0.84434951\n",
      "Iteration 16, loss = 0.83181262\n",
      "Iteration 17, loss = 0.81650819\n",
      "Iteration 18, loss = 0.80047590\n",
      "Iteration 19, loss = 0.78506703\n",
      "Iteration 20, loss = 0.77132040\n",
      "Iteration 21, loss = 0.75899877\n",
      "Iteration 22, loss = 0.74848286\n",
      "Iteration 23, loss = 0.73873752\n",
      "Iteration 24, loss = 0.72851108\n",
      "Iteration 25, loss = 0.71643654\n",
      "Iteration 26, loss = 0.70345681\n",
      "Iteration 27, loss = 0.69107105\n",
      "Iteration 28, loss = 0.68063261\n",
      "Iteration 29, loss = 0.67081395\n",
      "Iteration 30, loss = 0.66113956\n",
      "Iteration 31, loss = 0.65102452\n",
      "Iteration 32, loss = 0.64169962\n",
      "Iteration 33, loss = 0.63341455\n",
      "Iteration 34, loss = 0.62562268\n",
      "Iteration 35, loss = 0.61780877\n",
      "Iteration 36, loss = 0.60965285\n",
      "Iteration 37, loss = 0.60159439\n",
      "Iteration 38, loss = 0.59384908\n",
      "Iteration 39, loss = 0.58622000\n",
      "Iteration 40, loss = 0.57994490\n",
      "Iteration 41, loss = 0.57485736\n",
      "Iteration 42, loss = 0.56990184\n",
      "Iteration 43, loss = 0.56385720\n",
      "Iteration 44, loss = 0.55719444\n",
      "Iteration 45, loss = 0.55051973\n",
      "Iteration 46, loss = 0.54457733\n",
      "Iteration 47, loss = 0.53907527\n",
      "Iteration 48, loss = 0.53451300\n",
      "Iteration 49, loss = 0.53015256\n",
      "Iteration 50, loss = 0.52333008\n",
      "Iteration 51, loss = 0.51645268\n",
      "Iteration 52, loss = 0.51198661\n",
      "Iteration 53, loss = 0.50598274\n",
      "Iteration 54, loss = 0.49912391\n",
      "Iteration 55, loss = 0.49489095\n",
      "Iteration 56, loss = 0.48937313\n",
      "Iteration 57, loss = 0.48176141\n",
      "Iteration 58, loss = 0.47706710\n",
      "Iteration 59, loss = 0.47244097\n",
      "Iteration 60, loss = 0.46432838\n",
      "Iteration 61, loss = 0.45913566\n",
      "Iteration 62, loss = 0.45471919\n",
      "Iteration 63, loss = 0.44681108\n",
      "Iteration 64, loss = 0.44059995\n",
      "Iteration 65, loss = 0.43596879\n",
      "Iteration 66, loss = 0.42906691\n",
      "Iteration 67, loss = 0.42180135\n",
      "Iteration 68, loss = 0.41611738\n",
      "Iteration 69, loss = 0.41116470\n",
      "Iteration 70, loss = 0.40584124\n",
      "Iteration 71, loss = 0.39814219\n",
      "Iteration 72, loss = 0.39081015\n",
      "Iteration 73, loss = 0.38520763\n",
      "Iteration 74, loss = 0.38044662\n",
      "Iteration 75, loss = 0.37554200\n",
      "Iteration 76, loss = 0.36774735\n",
      "Iteration 77, loss = 0.35990167\n",
      "Iteration 78, loss = 0.35413282\n",
      "Iteration 79, loss = 0.34990601\n",
      "Iteration 80, loss = 0.34594856\n",
      "Iteration 81, loss = 0.33830224\n",
      "Iteration 82, loss = 0.33013214\n",
      "Iteration 83, loss = 0.32460606\n",
      "Iteration 84, loss = 0.32120252\n",
      "Iteration 85, loss = 0.31787757\n",
      "Iteration 86, loss = 0.31005599\n",
      "Iteration 87, loss = 0.30218218\n",
      "Iteration 88, loss = 0.29794977\n",
      "Iteration 89, loss = 0.29509035\n",
      "Iteration 90, loss = 0.29025099\n",
      "Iteration 91, loss = 0.28215946\n",
      "Iteration 92, loss = 0.27656308\n",
      "Iteration 93, loss = 0.27408288\n",
      "Iteration 94, loss = 0.27026212\n",
      "Iteration 95, loss = 0.26424677\n",
      "Iteration 96, loss = 0.25808237\n",
      "Iteration 97, loss = 0.25501273\n",
      "Iteration 98, loss = 0.25292618\n",
      "Iteration 99, loss = 0.24807311\n",
      "Iteration 100, loss = 0.24217127\n",
      "Iteration 101, loss = 0.23809674\n",
      "Iteration 102, loss = 0.23606019\n",
      "Iteration 103, loss = 0.23365527\n",
      "Iteration 104, loss = 0.22858983\n",
      "Iteration 105, loss = 0.22347931\n",
      "Iteration 106, loss = 0.22087251\n",
      "Iteration 107, loss = 0.21892218\n",
      "Iteration 108, loss = 0.21606638\n",
      "Iteration 109, loss = 0.21121562\n",
      "Iteration 110, loss = 0.20786111\n",
      "Iteration 111, loss = 0.20621066\n",
      "Iteration 112, loss = 0.20398796\n",
      "Iteration 113, loss = 0.20040716\n",
      "Iteration 114, loss = 0.19655950\n",
      "Iteration 115, loss = 0.19434809\n",
      "Iteration 116, loss = 0.19306664\n",
      "Iteration 117, loss = 0.19124299\n",
      "Iteration 118, loss = 0.18819538\n",
      "Iteration 119, loss = 0.18453403\n",
      "Iteration 120, loss = 0.18201641\n",
      "Iteration 121, loss = 0.18067902\n",
      "Iteration 122, loss = 0.17898630\n",
      "Iteration 123, loss = 0.17653543\n",
      "Iteration 124, loss = 0.17354361\n",
      "Iteration 125, loss = 0.17115589\n",
      "Iteration 126, loss = 0.16962472\n",
      "Iteration 127, loss = 0.16854807\n",
      "Iteration 128, loss = 0.16762101\n",
      "Iteration 129, loss = 0.16549679\n",
      "Iteration 130, loss = 0.16255824\n",
      "Iteration 131, loss = 0.15995319\n",
      "Iteration 132, loss = 0.15859975\n",
      "Iteration 133, loss = 0.15782211\n",
      "Iteration 134, loss = 0.15645985\n",
      "Iteration 135, loss = 0.15443118\n",
      "Iteration 136, loss = 0.15192178\n",
      "Iteration 137, loss = 0.15006193\n",
      "Iteration 138, loss = 0.14891443\n",
      "Iteration 139, loss = 0.14806513\n",
      "Iteration 140, loss = 0.14726848\n",
      "Iteration 141, loss = 0.14549215\n",
      "Iteration 142, loss = 0.14327292\n",
      "Iteration 143, loss = 0.14110354\n",
      "Iteration 144, loss = 0.13965282\n",
      "Iteration 145, loss = 0.13890110\n",
      "Iteration 146, loss = 0.13860190\n",
      "Iteration 147, loss = 0.13874774\n",
      "Iteration 148, loss = 0.13736204\n",
      "Iteration 149, loss = 0.13487868\n",
      "Iteration 150, loss = 0.13207892\n",
      "Iteration 151, loss = 0.13110038\n",
      "Iteration 152, loss = 0.13124787\n",
      "Iteration 153, loss = 0.13040211\n",
      "Iteration 154, loss = 0.12850274\n",
      "Iteration 155, loss = 0.12625726\n",
      "Iteration 156, loss = 0.12514733\n",
      "Iteration 157, loss = 0.12489280\n",
      "Iteration 158, loss = 0.12439009\n",
      "Iteration 159, loss = 0.12341897\n",
      "Iteration 160, loss = 0.12150450\n",
      "Iteration 161, loss = 0.11974632\n",
      "Iteration 162, loss = 0.11862084\n",
      "Iteration 163, loss = 0.11807236\n",
      "Iteration 164, loss = 0.11776478\n",
      "Iteration 165, loss = 0.11713495\n",
      "Iteration 166, loss = 0.11634395\n",
      "Iteration 167, loss = 0.11481054\n",
      "Iteration 168, loss = 0.11321501\n",
      "Iteration 169, loss = 0.11180739\n",
      "Iteration 170, loss = 0.11079201\n",
      "Iteration 171, loss = 0.11008475\n",
      "Iteration 172, loss = 0.10959495\n",
      "Iteration 173, loss = 0.10941870\n",
      "Iteration 174, loss = 0.10927611\n",
      "Iteration 175, loss = 0.10960320\n",
      "Iteration 176, loss = 0.10858199\n",
      "Iteration 177, loss = 0.10692691\n",
      "Iteration 178, loss = 0.10435560\n",
      "Iteration 179, loss = 0.10292805\n",
      "Iteration 180, loss = 0.10287377\n",
      "Iteration 181, loss = 0.10307943\n",
      "Iteration 182, loss = 0.10283322\n",
      "Iteration 183, loss = 0.10112683\n",
      "Iteration 184, loss = 0.09930799\n",
      "Iteration 185, loss = 0.09816002\n",
      "Iteration 186, loss = 0.09788716\n",
      "Iteration 187, loss = 0.09805283\n",
      "Iteration 188, loss = 0.09793435\n",
      "Iteration 189, loss = 0.09757262\n",
      "Iteration 190, loss = 0.09609939\n",
      "Iteration 191, loss = 0.09446165\n",
      "Iteration 192, loss = 0.09305688\n",
      "Iteration 193, loss = 0.09236084\n",
      "Iteration 194, loss = 0.09220994\n",
      "Iteration 195, loss = 0.09213593\n",
      "Iteration 196, loss = 0.09200731\n",
      "Iteration 197, loss = 0.09120614\n",
      "Iteration 198, loss = 0.09016867\n",
      "Iteration 199, loss = 0.08878555\n",
      "Iteration 200, loss = 0.08764668\n",
      "Iteration 201, loss = 0.08689131\n",
      "Iteration 202, loss = 0.08652638\n",
      "Iteration 203, loss = 0.08656629\n",
      "Iteration 204, loss = 0.08681953\n",
      "Iteration 205, loss = 0.08746118\n",
      "Iteration 206, loss = 0.08721618\n",
      "Iteration 207, loss = 0.08640439\n",
      "Iteration 208, loss = 0.08403182\n",
      "Iteration 209, loss = 0.08215608\n",
      "Iteration 210, loss = 0.08170651\n",
      "Iteration 211, loss = 0.08223529\n",
      "Iteration 212, loss = 0.08265351\n",
      "Iteration 213, loss = 0.08172193\n",
      "Iteration 214, loss = 0.08009255\n",
      "Iteration 215, loss = 0.07873406\n",
      "Iteration 216, loss = 0.07834971\n",
      "Iteration 217, loss = 0.07862191\n",
      "Iteration 218, loss = 0.07869684\n",
      "Iteration 219, loss = 0.07836348\n",
      "Iteration 220, loss = 0.07724389\n",
      "Iteration 221, loss = 0.07601367\n",
      "Iteration 222, loss = 0.07507940\n",
      "Iteration 223, loss = 0.07469144\n",
      "Iteration 224, loss = 0.07464667\n",
      "Iteration 225, loss = 0.07460044\n",
      "Iteration 226, loss = 0.07438539\n",
      "Iteration 227, loss = 0.07375779\n",
      "Iteration 228, loss = 0.07295519\n",
      "Iteration 229, loss = 0.07205279\n",
      "Iteration 230, loss = 0.07130666\n",
      "Iteration 231, loss = 0.07078977\n",
      "Iteration 232, loss = 0.07046580\n",
      "Iteration 233, loss = 0.07026761\n",
      "Iteration 234, loss = 0.07012734\n",
      "Iteration 235, loss = 0.07008664\n",
      "Iteration 236, loss = 0.07001486\n",
      "Iteration 237, loss = 0.07002451\n",
      "Iteration 238, loss = 0.06969324\n",
      "Iteration 239, loss = 0.06913983\n",
      "Iteration 240, loss = 0.06808658\n",
      "Iteration 241, loss = 0.06698622\n",
      "Iteration 242, loss = 0.06615005\n",
      "Iteration 243, loss = 0.06574868\n",
      "Iteration 244, loss = 0.06566652\n",
      "Iteration 245, loss = 0.06568052\n",
      "Iteration 246, loss = 0.06565312\n",
      "Iteration 247, loss = 0.06539829\n",
      "Iteration 248, loss = 0.06501598\n",
      "Iteration 249, loss = 0.06438927\n",
      "Iteration 250, loss = 0.06370432\n",
      "Iteration 251, loss = 0.06298374\n",
      "Iteration 252, loss = 0.06236129\n",
      "Iteration 253, loss = 0.06187151\n",
      "Iteration 254, loss = 0.06151752\n",
      "Iteration 255, loss = 0.06130782\n",
      "Iteration 256, loss = 0.06130864\n",
      "Iteration 257, loss = 0.06166849\n",
      "Iteration 258, loss = 0.06240204\n",
      "Iteration 259, loss = 0.06357875\n",
      "Iteration 260, loss = 0.06415349\n",
      "Iteration 261, loss = 0.06363621\n",
      "Iteration 262, loss = 0.06107406\n",
      "Iteration 263, loss = 0.05871326\n",
      "Iteration 264, loss = 0.05852389\n",
      "Iteration 265, loss = 0.05975321\n",
      "Iteration 266, loss = 0.06017213\n",
      "Iteration 267, loss = 0.05870870\n",
      "Iteration 268, loss = 0.05705535\n",
      "Iteration 269, loss = 0.05684861\n",
      "Iteration 270, loss = 0.05758317\n",
      "Iteration 271, loss = 0.05775261\n",
      "Iteration 272, loss = 0.05674405\n",
      "Iteration 273, loss = 0.05554211\n",
      "Iteration 274, loss = 0.05515031\n",
      "Iteration 275, loss = 0.05546022\n",
      "Iteration 276, loss = 0.05568507\n",
      "Iteration 277, loss = 0.05525751\n",
      "Iteration 278, loss = 0.05440842\n",
      "Iteration 279, loss = 0.05366112\n",
      "Iteration 280, loss = 0.05339943\n",
      "Iteration 281, loss = 0.05349808\n",
      "Iteration 282, loss = 0.05362891\n",
      "Iteration 283, loss = 0.05348779\n",
      "Iteration 284, loss = 0.05293963\n",
      "Iteration 285, loss = 0.05221554\n",
      "Iteration 286, loss = 0.05167842\n",
      "Iteration 287, loss = 0.05147482\n",
      "Iteration 288, loss = 0.05146375\n",
      "Iteration 289, loss = 0.05143344\n",
      "Iteration 290, loss = 0.05124520\n",
      "Iteration 291, loss = 0.05088086\n",
      "Iteration 292, loss = 0.05040417\n",
      "Iteration 293, loss = 0.04993191\n",
      "Iteration 294, loss = 0.04954043\n",
      "Iteration 295, loss = 0.04926005\n",
      "Iteration 296, loss = 0.04906458\n",
      "Iteration 297, loss = 0.04892017\n",
      "Iteration 298, loss = 0.04880736\n",
      "Iteration 299, loss = 0.04876842\n",
      "Iteration 300, loss = 0.04884722\n",
      "Iteration 301, loss = 0.04898829\n",
      "Iteration 302, loss = 0.04911699\n",
      "Iteration 303, loss = 0.04911097\n",
      "Iteration 304, loss = 0.04880252\n",
      "Iteration 305, loss = 0.04809868\n",
      "Iteration 306, loss = 0.04718788\n",
      "Iteration 307, loss = 0.04641067\n",
      "Iteration 308, loss = 0.04603510\n",
      "Iteration 309, loss = 0.04605599\n",
      "Iteration 310, loss = 0.04627516\n",
      "Iteration 311, loss = 0.04648539\n",
      "Iteration 312, loss = 0.04651286\n",
      "Iteration 313, loss = 0.04626832\n",
      "Iteration 314, loss = 0.04571684\n",
      "Iteration 315, loss = 0.04501705\n",
      "Iteration 316, loss = 0.04437416\n",
      "Iteration 317, loss = 0.04398914\n",
      "Iteration 318, loss = 0.04387841\n",
      "Iteration 319, loss = 0.04391645\n",
      "Iteration 320, loss = 0.04395486\n",
      "Iteration 321, loss = 0.04388396\n",
      "Iteration 322, loss = 0.04368156\n",
      "Iteration 323, loss = 0.04335051\n",
      "Iteration 324, loss = 0.04295753\n",
      "Iteration 325, loss = 0.04254630\n",
      "Iteration 326, loss = 0.04217607\n",
      "Iteration 327, loss = 0.04187565\n",
      "Iteration 328, loss = 0.04165356\n",
      "Iteration 329, loss = 0.04149284\n",
      "Iteration 330, loss = 0.04137264\n",
      "Iteration 331, loss = 0.04127990\n",
      "Iteration 332, loss = 0.04120990\n",
      "Iteration 333, loss = 0.04117299\n",
      "Iteration 334, loss = 0.04117412\n",
      "Iteration 335, loss = 0.04122547\n",
      "Iteration 336, loss = 0.04129130\n",
      "Iteration 337, loss = 0.04134892\n",
      "Iteration 338, loss = 0.04127079\n",
      "Iteration 339, loss = 0.04100399\n",
      "Iteration 340, loss = 0.04043903\n",
      "Iteration 341, loss = 0.03974479\n",
      "Iteration 342, loss = 0.03911128\n",
      "Iteration 343, loss = 0.03874913\n",
      "Iteration 344, loss = 0.03867805\n",
      "Iteration 345, loss = 0.03879526\n",
      "Iteration 346, loss = 0.03897249\n",
      "Iteration 347, loss = 0.03908014\n",
      "Iteration 348, loss = 0.03908097\n",
      "Iteration 349, loss = 0.03887978\n",
      "Iteration 350, loss = 0.03851538\n",
      "Iteration 351, loss = 0.03798275\n",
      "Iteration 352, loss = 0.03744447\n",
      "Iteration 353, loss = 0.03701809\n",
      "Iteration 354, loss = 0.03678825\n",
      "Iteration 355, loss = 0.03672765\n",
      "Iteration 356, loss = 0.03674594\n",
      "Iteration 357, loss = 0.03680809\n",
      "Iteration 358, loss = 0.03685939\n",
      "Iteration 359, loss = 0.03689833\n",
      "Iteration 360, loss = 0.03681575\n",
      "Iteration 361, loss = 0.03666845\n",
      "Iteration 362, loss = 0.03635359\n",
      "Iteration 363, loss = 0.03594836\n",
      "Iteration 364, loss = 0.03546455\n",
      "Iteration 365, loss = 0.03505569\n",
      "Iteration 366, loss = 0.03478675\n",
      "Iteration 367, loss = 0.03467969\n",
      "Iteration 368, loss = 0.03466500\n",
      "Iteration 369, loss = 0.03467144\n",
      "Iteration 370, loss = 0.03465755\n",
      "Iteration 371, loss = 0.03459132\n",
      "Iteration 372, loss = 0.03450083\n",
      "Iteration 373, loss = 0.03434508\n",
      "Iteration 374, loss = 0.03417258\n",
      "Iteration 375, loss = 0.03395222\n",
      "Iteration 376, loss = 0.03370595\n",
      "Iteration 377, loss = 0.03341210\n",
      "Iteration 378, loss = 0.03312020\n",
      "Iteration 379, loss = 0.03284794\n",
      "Iteration 380, loss = 0.03264024\n",
      "Iteration 381, loss = 0.03248249\n",
      "Iteration 382, loss = 0.03237442\n",
      "Iteration 383, loss = 0.03229094\n",
      "Iteration 384, loss = 0.03221913\n",
      "Iteration 385, loss = 0.03220229\n",
      "Iteration 386, loss = 0.03226937\n",
      "Iteration 387, loss = 0.03245284\n",
      "Iteration 388, loss = 0.03268680\n",
      "Iteration 389, loss = 0.03301868\n",
      "Iteration 390, loss = 0.03322071\n",
      "Iteration 391, loss = 0.03329538\n",
      "Iteration 392, loss = 0.03280069\n",
      "Iteration 393, loss = 0.03199810\n",
      "Iteration 394, loss = 0.03105228\n",
      "Iteration 395, loss = 0.03054342\n",
      "Iteration 396, loss = 0.03060244\n",
      "Iteration 397, loss = 0.03092207\n",
      "Iteration 398, loss = 0.03114970\n",
      "Iteration 399, loss = 0.03099523\n",
      "Iteration 400, loss = 0.03059658\n",
      "Iteration 401, loss = 0.03006126\n",
      "Iteration 402, loss = 0.02965593\n",
      "Iteration 403, loss = 0.02947836\n",
      "Iteration 404, loss = 0.02949234\n",
      "Iteration 405, loss = 0.02958288\n",
      "Iteration 406, loss = 0.02960607\n",
      "Iteration 407, loss = 0.02953610\n",
      "Iteration 408, loss = 0.02933389\n",
      "Iteration 409, loss = 0.02908046\n",
      "Iteration 410, loss = 0.02879724\n",
      "Iteration 411, loss = 0.02853214\n",
      "Iteration 412, loss = 0.02834688\n",
      "Iteration 413, loss = 0.02823787\n",
      "Iteration 414, loss = 0.02817810\n",
      "Iteration 415, loss = 0.02814481\n",
      "Iteration 416, loss = 0.02809451\n",
      "Iteration 417, loss = 0.02801704\n",
      "Iteration 418, loss = 0.02791192\n",
      "Iteration 419, loss = 0.02776907\n",
      "Iteration 420, loss = 0.02761084\n",
      "Iteration 421, loss = 0.02742832\n",
      "Iteration 422, loss = 0.02725517\n",
      "Iteration 423, loss = 0.02708700\n",
      "Iteration 424, loss = 0.02693977\n",
      "Iteration 425, loss = 0.02680664\n",
      "Iteration 426, loss = 0.02668646\n",
      "Iteration 427, loss = 0.02657830\n",
      "Iteration 428, loss = 0.02647581\n",
      "Iteration 429, loss = 0.02638392\n",
      "Iteration 430, loss = 0.02630262\n",
      "Iteration 431, loss = 0.02625202\n",
      "Iteration 432, loss = 0.02624051\n",
      "Iteration 433, loss = 0.02631880\n",
      "Iteration 434, loss = 0.02650196\n",
      "Iteration 435, loss = 0.02685786\n",
      "Iteration 436, loss = 0.02732403\n",
      "Iteration 437, loss = 0.02799524\n",
      "Iteration 438, loss = 0.02841788\n",
      "Iteration 439, loss = 0.02863242\n",
      "Iteration 440, loss = 0.02778086\n",
      "Iteration 441, loss = 0.02645204\n",
      "Iteration 442, loss = 0.02517052\n",
      "Iteration 443, loss = 0.02502722\n",
      "Iteration 444, loss = 0.02574150\n",
      "Iteration 445, loss = 0.02616453\n",
      "Iteration 446, loss = 0.02592394\n",
      "Iteration 447, loss = 0.02503676\n",
      "Iteration 448, loss = 0.02439271\n",
      "Iteration 449, loss = 0.02437557\n",
      "Iteration 450, loss = 0.02473923\n",
      "Iteration 451, loss = 0.02502317\n",
      "Iteration 452, loss = 0.02484526\n",
      "Iteration 453, loss = 0.02436990\n",
      "Iteration 454, loss = 0.02386167\n",
      "Iteration 455, loss = 0.02366308\n",
      "Iteration 456, loss = 0.02378371\n",
      "Iteration 457, loss = 0.02397038\n",
      "Iteration 458, loss = 0.02398148\n",
      "Iteration 459, loss = 0.02369415\n",
      "Iteration 460, loss = 0.02332910\n",
      "Iteration 461, loss = 0.02308868\n",
      "Iteration 462, loss = 0.02305640\n",
      "Iteration 463, loss = 0.02315572\n",
      "Iteration 464, loss = 0.02324009\n",
      "Iteration 465, loss = 0.02321091\n",
      "Iteration 466, loss = 0.02298643\n",
      "Iteration 467, loss = 0.02270092\n",
      "Iteration 468, loss = 0.02246505\n",
      "Iteration 469, loss = 0.02237184\n",
      "Iteration 470, loss = 0.02238160\n",
      "Iteration 471, loss = 0.02238998\n",
      "Iteration 472, loss = 0.02232882\n",
      "Iteration 473, loss = 0.02217600\n",
      "Iteration 474, loss = 0.02199567\n",
      "Iteration 475, loss = 0.02184004\n",
      "Iteration 476, loss = 0.02174543\n",
      "Iteration 477, loss = 0.02169907\n",
      "Iteration 478, loss = 0.02166401\n",
      "Iteration 479, loss = 0.02160896\n",
      "Iteration 480, loss = 0.02151415\n",
      "Iteration 481, loss = 0.02139554\n",
      "Iteration 482, loss = 0.02126793\n",
      "Iteration 483, loss = 0.02115346\n",
      "Iteration 484, loss = 0.02106199\n",
      "Iteration 485, loss = 0.02099101\n",
      "Iteration 486, loss = 0.02093046\n",
      "Iteration 487, loss = 0.02086620\n",
      "Iteration 488, loss = 0.02079187\n",
      "Iteration 489, loss = 0.02070423\n",
      "Iteration 490, loss = 0.02060882\n",
      "Iteration 491, loss = 0.02051038\n",
      "Iteration 492, loss = 0.02041507\n",
      "Iteration 493, loss = 0.02032631\n",
      "Iteration 494, loss = 0.02024555\n",
      "Iteration 495, loss = 0.02017042\n",
      "Iteration 496, loss = 0.02009769\n",
      "Iteration 497, loss = 0.02002615\n",
      "Iteration 498, loss = 0.01995459\n",
      "Iteration 499, loss = 0.01988689\n",
      "Iteration 500, loss = 0.01983497\n",
      "Iteration 501, loss = 0.01980045\n",
      "Iteration 502, loss = 0.01980290\n",
      "Iteration 503, loss = 0.01983396\n",
      "Iteration 504, loss = 0.01988215\n",
      "Iteration 505, loss = 0.01993396\n",
      "Iteration 506, loss = 0.01993650\n",
      "Iteration 507, loss = 0.01988544\n",
      "Iteration 508, loss = 0.01971782\n",
      "Iteration 509, loss = 0.01948616\n",
      "Iteration 510, loss = 0.01921553\n",
      "Iteration 511, loss = 0.01899854\n",
      "Iteration 512, loss = 0.01887737\n",
      "Iteration 513, loss = 0.01884899\n",
      "Iteration 514, loss = 0.01887345\n",
      "Iteration 515, loss = 0.01890151\n",
      "Iteration 516, loss = 0.01891004\n",
      "Iteration 517, loss = 0.01886982\n",
      "Iteration 518, loss = 0.01878721\n",
      "Iteration 519, loss = 0.01864730\n",
      "Iteration 520, loss = 0.01848345\n",
      "Iteration 521, loss = 0.01831686\n",
      "Iteration 522, loss = 0.01818446\n",
      "Iteration 523, loss = 0.01809944\n",
      "Iteration 524, loss = 0.01805540\n",
      "Iteration 525, loss = 0.01803396\n",
      "Iteration 526, loss = 0.01801438\n",
      "Iteration 527, loss = 0.01798731\n",
      "Iteration 528, loss = 0.01794109\n",
      "Iteration 529, loss = 0.01787854\n",
      "Iteration 530, loss = 0.01779289\n",
      "Iteration 531, loss = 0.01769411\n",
      "Iteration 532, loss = 0.01758452\n",
      "Iteration 533, loss = 0.01747770\n",
      "Iteration 534, loss = 0.01738052\n",
      "Iteration 535, loss = 0.01729864\n",
      "Iteration 536, loss = 0.01723161\n",
      "Iteration 537, loss = 0.01717631\n",
      "Iteration 538, loss = 0.01712905\n",
      "Iteration 539, loss = 0.01708502\n",
      "Iteration 540, loss = 0.01704169\n",
      "Iteration 541, loss = 0.01699434\n",
      "Iteration 542, loss = 0.01694263\n",
      "Iteration 543, loss = 0.01688345\n",
      "Iteration 544, loss = 0.01681877\n",
      "Iteration 545, loss = 0.01674691\n",
      "Iteration 546, loss = 0.01667135\n",
      "Iteration 547, loss = 0.01659223\n",
      "Iteration 548, loss = 0.01651317\n",
      "Iteration 549, loss = 0.01643563\n",
      "Iteration 550, loss = 0.01636197\n",
      "Iteration 551, loss = 0.01629231\n",
      "Iteration 552, loss = 0.01622629\n",
      "Iteration 553, loss = 0.01616324\n",
      "Iteration 554, loss = 0.01610401\n",
      "Iteration 555, loss = 0.01604742\n",
      "Iteration 556, loss = 0.01599183\n",
      "Iteration 557, loss = 0.01593665\n",
      "Iteration 558, loss = 0.01588270\n",
      "Iteration 559, loss = 0.01583218\n",
      "Iteration 560, loss = 0.01578395\n",
      "Iteration 561, loss = 0.01573804\n",
      "Iteration 562, loss = 0.01569463\n",
      "Iteration 563, loss = 0.01565627\n",
      "Iteration 564, loss = 0.01562218\n",
      "Iteration 565, loss = 0.01559225\n",
      "Iteration 566, loss = 0.01556417\n",
      "Iteration 567, loss = 0.01554194\n",
      "Iteration 568, loss = 0.01551830\n",
      "Iteration 569, loss = 0.01549674\n",
      "Iteration 570, loss = 0.01546474\n",
      "Iteration 571, loss = 0.01542609\n",
      "Iteration 572, loss = 0.01536311\n",
      "Iteration 573, loss = 0.01528396\n",
      "Iteration 574, loss = 0.01517613\n",
      "Iteration 575, loss = 0.01505609\n",
      "Iteration 576, loss = 0.01493185\n",
      "Iteration 577, loss = 0.01482248\n",
      "Iteration 578, loss = 0.01473646\n",
      "Iteration 579, loss = 0.01467585\n",
      "Iteration 580, loss = 0.01463543\n",
      "Iteration 581, loss = 0.01460721\n",
      "Iteration 582, loss = 0.01458369\n",
      "Iteration 583, loss = 0.01455870\n",
      "Iteration 584, loss = 0.01452743\n",
      "Iteration 585, loss = 0.01448358\n",
      "Iteration 586, loss = 0.01442831\n",
      "Iteration 587, loss = 0.01435999\n",
      "Iteration 588, loss = 0.01428504\n",
      "Iteration 589, loss = 0.01420678\n",
      "Iteration 590, loss = 0.01413139\n",
      "Iteration 591, loss = 0.01406261\n",
      "Iteration 592, loss = 0.01400225\n",
      "Iteration 593, loss = 0.01395031\n",
      "Iteration 594, loss = 0.01390462\n",
      "Iteration 595, loss = 0.01386266\n",
      "Iteration 596, loss = 0.01382203\n",
      "Iteration 597, loss = 0.01378097\n",
      "Iteration 598, loss = 0.01373768\n",
      "Iteration 599, loss = 0.01369150\n",
      "Iteration 600, loss = 0.01364230\n",
      "Iteration 601, loss = 0.01359034\n",
      "Iteration 602, loss = 0.01353585\n",
      "Iteration 603, loss = 0.01348037\n",
      "Iteration 604, loss = 0.01342515\n",
      "Iteration 605, loss = 0.01337100\n",
      "Iteration 606, loss = 0.01331889\n",
      "Iteration 607, loss = 0.01326906\n",
      "Iteration 608, loss = 0.01322122\n",
      "Iteration 609, loss = 0.01317496\n",
      "Iteration 610, loss = 0.01313006\n",
      "Iteration 611, loss = 0.01308562\n",
      "Iteration 612, loss = 0.01304137\n",
      "Iteration 613, loss = 0.01299706\n",
      "Iteration 614, loss = 0.01295227\n",
      "Iteration 615, loss = 0.01290693\n",
      "Iteration 616, loss = 0.01286113\n",
      "Iteration 617, loss = 0.01281493\n",
      "Iteration 618, loss = 0.01276842\n",
      "Iteration 619, loss = 0.01272200\n",
      "Iteration 620, loss = 0.01267574\n",
      "Iteration 621, loss = 0.01262978\n",
      "Iteration 622, loss = 0.01258432\n",
      "Iteration 623, loss = 0.01253937\n",
      "Iteration 624, loss = 0.01249487\n",
      "Iteration 625, loss = 0.01245088\n",
      "Iteration 626, loss = 0.01240729\n",
      "Iteration 627, loss = 0.01236405\n",
      "Iteration 628, loss = 0.01232111\n",
      "Iteration 629, loss = 0.01227840\n",
      "Iteration 630, loss = 0.01223583\n",
      "Iteration 631, loss = 0.01219343\n",
      "Iteration 632, loss = 0.01215108\n",
      "Iteration 633, loss = 0.01210885\n",
      "Iteration 634, loss = 0.01206669\n",
      "Iteration 635, loss = 0.01202465\n",
      "Iteration 636, loss = 0.01198271\n",
      "Iteration 637, loss = 0.01194094\n",
      "Iteration 638, loss = 0.01189928\n",
      "Iteration 639, loss = 0.01185783\n",
      "Iteration 640, loss = 0.01181654\n",
      "Iteration 641, loss = 0.01177546\n",
      "Iteration 642, loss = 0.01173457\n",
      "Iteration 643, loss = 0.01169389\n",
      "Iteration 644, loss = 0.01165342\n",
      "Iteration 645, loss = 0.01161314\n",
      "Iteration 646, loss = 0.01157307\n",
      "Iteration 647, loss = 0.01153317\n",
      "Iteration 648, loss = 0.01149347\n",
      "Iteration 649, loss = 0.01145393\n",
      "Iteration 650, loss = 0.01141458\n",
      "Iteration 651, loss = 0.01137537\n",
      "Iteration 652, loss = 0.01133633\n",
      "Iteration 653, loss = 0.01129745\n",
      "Iteration 654, loss = 0.01125871\n",
      "Iteration 655, loss = 0.01122011\n",
      "Iteration 656, loss = 0.01118149\n",
      "Iteration 657, loss = 0.01114228\n",
      "Iteration 658, loss = 0.01110516\n",
      "Iteration 659, loss = 0.01106921\n",
      "Iteration 660, loss = 0.01103885\n",
      "Iteration 661, loss = 0.01100926\n",
      "Iteration 662, loss = 0.01098117\n",
      "Iteration 663, loss = 0.01095854\n",
      "Iteration 664, loss = 0.01094354\n",
      "Iteration 665, loss = 0.01094782\n",
      "Iteration 666, loss = 0.01095939\n",
      "Iteration 667, loss = 0.01096930\n",
      "Iteration 668, loss = 0.01096639\n",
      "Iteration 669, loss = 0.01094757\n",
      "Iteration 670, loss = 0.01089952\n",
      "Iteration 671, loss = 0.01082110\n",
      "Iteration 672, loss = 0.01072460\n",
      "Iteration 673, loss = 0.01062768\n",
      "Iteration 674, loss = 0.01054401\n",
      "Iteration 675, loss = 0.01048070\n",
      "Iteration 676, loss = 0.01043476\n",
      "Iteration 677, loss = 0.01040280\n",
      "Iteration 678, loss = 0.01038587\n",
      "Iteration 679, loss = 0.01039685\n",
      "Iteration 680, loss = 0.01046098\n",
      "Iteration 681, loss = 0.01056269\n",
      "Iteration 682, loss = 0.01063730\n",
      "Iteration 683, loss = 0.01063824\n",
      "Iteration 684, loss = 0.01053924\n",
      "Iteration 685, loss = 0.01036706\n",
      "Iteration 686, loss = 0.01018339\n",
      "Iteration 687, loss = 0.01006486\n",
      "Iteration 688, loss = 0.01004275\n",
      "Iteration 689, loss = 0.01010601\n",
      "Iteration 690, loss = 0.01022184\n",
      "Iteration 691, loss = 0.01037127\n",
      "Iteration 692, loss = 0.01045653\n",
      "Iteration 693, loss = 0.01037951\n",
      "Iteration 694, loss = 0.01015745\n",
      "Iteration 695, loss = 0.00989579\n",
      "Iteration 696, loss = 0.00976624\n",
      "Iteration 697, loss = 0.00981113\n",
      "Iteration 698, loss = 0.00991751\n",
      "Iteration 699, loss = 0.00995651\n",
      "Iteration 700, loss = 0.00987249\n",
      "Iteration 701, loss = 0.00971758\n",
      "Iteration 702, loss = 0.00958219\n",
      "Iteration 703, loss = 0.00953684\n",
      "Iteration 704, loss = 0.00957620\n",
      "Iteration 705, loss = 0.00967328\n",
      "Iteration 706, loss = 0.00976774\n",
      "Iteration 707, loss = 0.00980300\n",
      "Iteration 708, loss = 0.00970973\n",
      "Iteration 709, loss = 0.00950775\n",
      "Iteration 710, loss = 0.00934026\n",
      "Iteration 711, loss = 0.00930901\n",
      "Iteration 712, loss = 0.00937726\n",
      "Iteration 713, loss = 0.00941882\n",
      "Iteration 714, loss = 0.00935805\n",
      "Iteration 715, loss = 0.00923488\n",
      "Iteration 716, loss = 0.00914200\n",
      "Iteration 717, loss = 0.00912642\n",
      "Iteration 718, loss = 0.00916970\n",
      "Iteration 719, loss = 0.00921715\n",
      "Iteration 720, loss = 0.00921633\n",
      "Iteration 721, loss = 0.00914111\n",
      "Iteration 722, loss = 0.00902749\n",
      "Iteration 723, loss = 0.00893833\n",
      "Iteration 724, loss = 0.00890695\n",
      "Iteration 725, loss = 0.00891893\n",
      "Iteration 726, loss = 0.00892824\n",
      "Iteration 727, loss = 0.00889866\n",
      "Iteration 728, loss = 0.00883506\n",
      "Iteration 729, loss = 0.00876916\n",
      "Iteration 730, loss = 0.00873148\n",
      "Iteration 731, loss = 0.00872243\n",
      "Iteration 732, loss = 0.00871700\n",
      "Iteration 733, loss = 0.00869303\n",
      "Iteration 734, loss = 0.00864857\n",
      "Iteration 735, loss = 0.00860016\n",
      "Iteration 736, loss = 0.00856367\n",
      "Iteration 737, loss = 0.00854275\n",
      "Iteration 738, loss = 0.00852826\n",
      "Iteration 739, loss = 0.00850832\n",
      "Iteration 740, loss = 0.00847839\n",
      "Iteration 741, loss = 0.00844086\n",
      "Iteration 742, loss = 0.00840424\n",
      "Iteration 743, loss = 0.00837471\n",
      "Iteration 744, loss = 0.00835237\n",
      "Iteration 745, loss = 0.00833202\n",
      "Iteration 746, loss = 0.00830812\n",
      "Iteration 747, loss = 0.00827937\n",
      "Iteration 748, loss = 0.00824758\n",
      "Iteration 749, loss = 0.00821582\n",
      "Iteration 750, loss = 0.00818705\n",
      "Iteration 751, loss = 0.00816119\n",
      "Iteration 752, loss = 0.00813844\n",
      "Iteration 753, loss = 0.00811510\n",
      "Iteration 754, loss = 0.00809106\n",
      "Iteration 755, loss = 0.00806939\n",
      "Iteration 756, loss = 0.00804499\n",
      "Iteration 757, loss = 0.00802365\n",
      "Iteration 758, loss = 0.00799529\n",
      "Iteration 759, loss = 0.00796435\n",
      "Iteration 760, loss = 0.00793280\n",
      "Iteration 761, loss = 0.00790343\n",
      "Iteration 762, loss = 0.00787958\n",
      "Iteration 763, loss = 0.00785802\n",
      "Iteration 764, loss = 0.00783609\n",
      "Iteration 765, loss = 0.00781457\n",
      "Iteration 766, loss = 0.00779222\n",
      "Iteration 767, loss = 0.00776734\n",
      "Iteration 768, loss = 0.00773908\n",
      "Iteration 769, loss = 0.00770952\n",
      "Iteration 770, loss = 0.00768167\n",
      "Iteration 771, loss = 0.00765582\n",
      "Iteration 772, loss = 0.00763093\n",
      "Iteration 773, loss = 0.00760743\n",
      "Iteration 774, loss = 0.00758522\n",
      "Iteration 775, loss = 0.00756347\n",
      "Iteration 776, loss = 0.00754163\n",
      "Iteration 777, loss = 0.00751951\n",
      "Iteration 778, loss = 0.00749788\n",
      "Iteration 779, loss = 0.00747523\n",
      "Iteration 780, loss = 0.00745208\n",
      "Iteration 781, loss = 0.00742763\n",
      "Iteration 782, loss = 0.00740329\n",
      "Iteration 783, loss = 0.00737809\n",
      "Iteration 784, loss = 0.00735324\n",
      "Iteration 785, loss = 0.00732856\n",
      "Iteration 786, loss = 0.00730465\n",
      "Iteration 787, loss = 0.00728099\n",
      "Iteration 788, loss = 0.00725792\n",
      "Iteration 789, loss = 0.00723529\n",
      "Iteration 790, loss = 0.00721339\n",
      "Iteration 791, loss = 0.00719187\n",
      "Iteration 792, loss = 0.00717165\n",
      "Iteration 793, loss = 0.00715250\n",
      "Iteration 794, loss = 0.00713637\n",
      "Iteration 795, loss = 0.00712145\n",
      "Iteration 796, loss = 0.00711144\n",
      "Iteration 797, loss = 0.00709830\n",
      "Iteration 798, loss = 0.00708635\n",
      "Iteration 799, loss = 0.00706080\n",
      "Iteration 800, loss = 0.00702428\n",
      "Iteration 801, loss = 0.00700107\n",
      "Iteration 802, loss = 0.00699134\n",
      "Iteration 803, loss = 0.00698570\n",
      "Iteration 804, loss = 0.00697416\n",
      "Iteration 805, loss = 0.00694268\n",
      "Iteration 806, loss = 0.00690377\n",
      "Iteration 807, loss = 0.00685662\n",
      "Iteration 808, loss = 0.00681741\n",
      "Iteration 809, loss = 0.00679211\n",
      "Iteration 810, loss = 0.00677972\n",
      "Iteration 811, loss = 0.00677487\n",
      "Iteration 812, loss = 0.00677124\n",
      "Iteration 813, loss = 0.00677044\n",
      "Iteration 814, loss = 0.00675729\n",
      "Iteration 815, loss = 0.00673464\n",
      "Iteration 816, loss = 0.00669053\n",
      "Iteration 817, loss = 0.00664300\n",
      "Iteration 818, loss = 0.00660736\n",
      "Iteration 819, loss = 0.00659186\n",
      "Iteration 820, loss = 0.00658825\n",
      "Iteration 821, loss = 0.00658378\n",
      "Iteration 822, loss = 0.00657450\n",
      "Iteration 823, loss = 0.00655212\n",
      "Iteration 824, loss = 0.00652366\n",
      "Iteration 825, loss = 0.00648785\n",
      "Iteration 826, loss = 0.00645377\n",
      "Iteration 827, loss = 0.00642639\n",
      "Iteration 828, loss = 0.00640839\n",
      "Iteration 829, loss = 0.00639638\n",
      "Iteration 830, loss = 0.00638449\n",
      "Iteration 831, loss = 0.00636948\n",
      "Iteration 832, loss = 0.00634825\n",
      "Iteration 833, loss = 0.00632378\n",
      "Iteration 834, loss = 0.00629712\n",
      "Iteration 835, loss = 0.00627219\n",
      "Iteration 836, loss = 0.00625071\n",
      "Iteration 837, loss = 0.00623280\n",
      "Iteration 838, loss = 0.00621676\n",
      "Iteration 839, loss = 0.00620064\n",
      "Iteration 840, loss = 0.00618322\n",
      "Iteration 841, loss = 0.00616468\n",
      "Iteration 842, loss = 0.00614523\n",
      "Iteration 843, loss = 0.00612490\n",
      "Iteration 844, loss = 0.00610436\n",
      "Iteration 845, loss = 0.00608370\n",
      "Iteration 846, loss = 0.00606347\n",
      "Iteration 847, loss = 0.00604411\n",
      "Iteration 848, loss = 0.00602564\n",
      "Iteration 849, loss = 0.00600790\n",
      "Iteration 850, loss = 0.00599057\n",
      "Iteration 851, loss = 0.00597331\n",
      "Iteration 852, loss = 0.00595581\n",
      "Iteration 853, loss = 0.00593805\n",
      "Iteration 854, loss = 0.00591997\n",
      "Iteration 855, loss = 0.00590172\n",
      "Iteration 856, loss = 0.00588335\n",
      "Iteration 857, loss = 0.00586499\n",
      "Iteration 858, loss = 0.00584669\n",
      "Iteration 859, loss = 0.00582855\n",
      "Iteration 860, loss = 0.00581063\n",
      "Iteration 861, loss = 0.00579295\n",
      "Iteration 862, loss = 0.00577548\n",
      "Iteration 863, loss = 0.00575815\n",
      "Iteration 864, loss = 0.00574092\n",
      "Iteration 865, loss = 0.00572378\n",
      "Iteration 866, loss = 0.00570671\n",
      "Iteration 867, loss = 0.00568966\n",
      "Iteration 868, loss = 0.00567268\n",
      "Iteration 869, loss = 0.00565578\n",
      "Iteration 870, loss = 0.00563899\n",
      "Iteration 871, loss = 0.00562228\n",
      "Iteration 872, loss = 0.00560572\n",
      "Iteration 873, loss = 0.00558930\n",
      "Iteration 874, loss = 0.00557317\n",
      "Iteration 875, loss = 0.00555724\n",
      "Iteration 876, loss = 0.00554178\n",
      "Iteration 877, loss = 0.00552651\n",
      "Iteration 878, loss = 0.00551200\n",
      "Iteration 879, loss = 0.00549760\n",
      "Iteration 880, loss = 0.00548448\n",
      "Iteration 881, loss = 0.00547111\n",
      "Iteration 882, loss = 0.00545936\n",
      "Iteration 883, loss = 0.00544728\n",
      "Iteration 884, loss = 0.00543820\n",
      "Iteration 885, loss = 0.00542508\n",
      "Iteration 886, loss = 0.00541126\n",
      "Iteration 887, loss = 0.00538859\n",
      "Iteration 888, loss = 0.00536416\n",
      "Iteration 889, loss = 0.00533707\n",
      "Iteration 890, loss = 0.00531344\n",
      "Iteration 891, loss = 0.00529447\n",
      "Iteration 892, loss = 0.00528002\n",
      "Iteration 893, loss = 0.00526884\n",
      "Iteration 894, loss = 0.00525954\n",
      "Iteration 895, loss = 0.00525242\n",
      "Iteration 896, loss = 0.00524441\n",
      "Iteration 897, loss = 0.00523781\n",
      "Iteration 898, loss = 0.00522349\n",
      "Iteration 899, loss = 0.00520519\n",
      "Iteration 900, loss = 0.00517719\n",
      "Iteration 901, loss = 0.00514930\n",
      "Iteration 902, loss = 0.00512654\n",
      "Iteration 903, loss = 0.00511214\n",
      "Iteration 904, loss = 0.00510370\n",
      "Iteration 905, loss = 0.00509673\n",
      "Iteration 906, loss = 0.00508937\n",
      "Iteration 907, loss = 0.00507706\n",
      "Iteration 908, loss = 0.00506262\n",
      "Iteration 909, loss = 0.00504179\n",
      "Iteration 910, loss = 0.00501955\n",
      "Iteration 911, loss = 0.00499697\n",
      "Iteration 912, loss = 0.00497797\n",
      "Iteration 913, loss = 0.00496346\n",
      "Iteration 914, loss = 0.00495188\n",
      "Iteration 915, loss = 0.00494235\n",
      "Iteration 916, loss = 0.00493323\n",
      "Iteration 917, loss = 0.00492186\n",
      "Iteration 918, loss = 0.00490599\n",
      "Iteration 919, loss = 0.00488810\n",
      "Iteration 920, loss = 0.00486875\n",
      "Iteration 921, loss = 0.00485086\n",
      "Iteration 922, loss = 0.00483555\n",
      "Iteration 923, loss = 0.00482278\n",
      "Iteration 924, loss = 0.00481139\n",
      "Iteration 925, loss = 0.00479981\n",
      "Iteration 926, loss = 0.00478729\n",
      "Iteration 927, loss = 0.00477278\n",
      "Iteration 928, loss = 0.00475661\n",
      "Iteration 929, loss = 0.00474144\n",
      "Iteration 930, loss = 0.00472800\n",
      "Iteration 931, loss = 0.00471453\n",
      "Iteration 932, loss = 0.00470056\n",
      "Iteration 933, loss = 0.00468581\n",
      "Iteration 934, loss = 0.00467119\n",
      "Iteration 935, loss = 0.00465702\n",
      "Iteration 936, loss = 0.00464353\n",
      "Iteration 937, loss = 0.00463030\n",
      "Iteration 938, loss = 0.00461692\n",
      "Iteration 939, loss = 0.00460379\n",
      "Iteration 940, loss = 0.00459156\n",
      "Iteration 941, loss = 0.00457949\n",
      "Iteration 942, loss = 0.00456826\n",
      "Iteration 943, loss = 0.00455617\n",
      "Iteration 944, loss = 0.00454553\n",
      "Iteration 945, loss = 0.00453351\n",
      "Iteration 946, loss = 0.00452276\n",
      "Iteration 947, loss = 0.00450859\n",
      "Iteration 948, loss = 0.00449361\n",
      "Iteration 949, loss = 0.00447641\n",
      "Iteration 950, loss = 0.00446036\n",
      "Iteration 951, loss = 0.00444396\n",
      "Iteration 952, loss = 0.00442840\n",
      "Iteration 953, loss = 0.00441370\n",
      "Iteration 954, loss = 0.00439999\n",
      "Iteration 955, loss = 0.00438698\n",
      "Iteration 956, loss = 0.00437454\n",
      "Iteration 957, loss = 0.00436264\n",
      "Iteration 958, loss = 0.00435122\n",
      "Iteration 959, loss = 0.00434097\n",
      "Iteration 960, loss = 0.00433109\n",
      "Iteration 961, loss = 0.00432310\n",
      "Iteration 962, loss = 0.00431328\n",
      "Iteration 963, loss = 0.00430451\n",
      "Iteration 964, loss = 0.00429080\n",
      "Iteration 965, loss = 0.00427695\n",
      "Iteration 966, loss = 0.00425810\n",
      "Iteration 967, loss = 0.00423944\n",
      "Iteration 968, loss = 0.00422167\n",
      "Iteration 969, loss = 0.00420664\n",
      "Iteration 970, loss = 0.00419410\n",
      "Iteration 971, loss = 0.00418344\n",
      "Iteration 972, loss = 0.00417420\n",
      "Iteration 973, loss = 0.00416561\n",
      "Iteration 974, loss = 0.00415914\n",
      "Iteration 975, loss = 0.00415108\n",
      "Iteration 976, loss = 0.00414424\n",
      "Iteration 977, loss = 0.00413106\n",
      "Iteration 978, loss = 0.00411722\n",
      "Iteration 979, loss = 0.00409662\n",
      "Iteration 980, loss = 0.00407640\n",
      "Iteration 981, loss = 0.00405853\n",
      "Iteration 982, loss = 0.00404538\n",
      "Iteration 983, loss = 0.00403613\n",
      "Iteration 984, loss = 0.00402869\n",
      "Iteration 985, loss = 0.00402211\n",
      "Iteration 986, loss = 0.00401304\n",
      "Iteration 987, loss = 0.00400397\n",
      "Iteration 988, loss = 0.00399004\n",
      "Iteration 989, loss = 0.00397541\n",
      "Iteration 990, loss = 0.00395767\n",
      "Iteration 991, loss = 0.00394099\n",
      "Iteration 992, loss = 0.00392592\n",
      "Iteration 993, loss = 0.00391348\n",
      "Iteration 994, loss = 0.00390319\n",
      "Iteration 995, loss = 0.00389389\n",
      "Iteration 996, loss = 0.00388481\n",
      "Iteration 997, loss = 0.00387451\n",
      "Iteration 998, loss = 0.00386381\n",
      "Iteration 999, loss = 0.00385125\n",
      "Iteration 1000, loss = 0.00383848\n",
      "Iteration 1001, loss = 0.00382487\n",
      "Iteration 1002, loss = 0.00381167\n",
      "Iteration 1003, loss = 0.00379888\n",
      "Iteration 1004, loss = 0.00378684\n",
      "Iteration 1005, loss = 0.00377551\n",
      "Iteration 1006, loss = 0.00376473\n",
      "Iteration 1007, loss = 0.00375431\n",
      "Iteration 1008, loss = 0.00374398\n",
      "Iteration 1009, loss = 0.00373373\n",
      "Iteration 1010, loss = 0.00372319\n",
      "Iteration 1011, loss = 0.00371270\n",
      "Iteration 1012, loss = 0.00370175\n",
      "Iteration 1013, loss = 0.00369085\n",
      "Iteration 1014, loss = 0.00367954\n",
      "Iteration 1015, loss = 0.00366834\n",
      "Iteration 1016, loss = 0.00365692\n",
      "Iteration 1017, loss = 0.00364566\n",
      "Iteration 1018, loss = 0.00363442\n",
      "Iteration 1019, loss = 0.00362337\n",
      "Iteration 1020, loss = 0.00361246\n",
      "Iteration 1021, loss = 0.00360172\n",
      "Iteration 1022, loss = 0.00359110\n",
      "Iteration 1023, loss = 0.00358061\n",
      "Iteration 1024, loss = 0.00357021\n",
      "Iteration 1025, loss = 0.00355990\n",
      "Iteration 1026, loss = 0.00354966\n",
      "Iteration 1027, loss = 0.00353949\n",
      "Iteration 1028, loss = 0.00352940\n",
      "Iteration 1029, loss = 0.00351937\n",
      "Iteration 1030, loss = 0.00350945\n",
      "Iteration 1031, loss = 0.00349960\n",
      "Iteration 1032, loss = 0.00348994\n",
      "Iteration 1033, loss = 0.00348034\n",
      "Iteration 1034, loss = 0.00347105\n",
      "Iteration 1035, loss = 0.00346177\n",
      "Iteration 1036, loss = 0.00345298\n",
      "Iteration 1037, loss = 0.00344401\n",
      "Iteration 1038, loss = 0.00343566\n",
      "Iteration 1039, loss = 0.00342661\n",
      "Iteration 1040, loss = 0.00341810\n",
      "Iteration 1041, loss = 0.00340817\n",
      "Iteration 1042, loss = 0.00339846\n",
      "Iteration 1043, loss = 0.00338720\n",
      "Iteration 1044, loss = 0.00337607\n",
      "Iteration 1045, loss = 0.00336437\n",
      "Iteration 1046, loss = 0.00335318\n",
      "Iteration 1047, loss = 0.00334248\n",
      "Iteration 1048, loss = 0.00333248\n",
      "Iteration 1049, loss = 0.00332310\n",
      "Iteration 1050, loss = 0.00331418\n",
      "Iteration 1051, loss = 0.00330556\n",
      "Iteration 1052, loss = 0.00329704\n",
      "Iteration 1053, loss = 0.00328862\n",
      "Iteration 1054, loss = 0.00327997\n",
      "Iteration 1055, loss = 0.00327132\n",
      "Iteration 1056, loss = 0.00326220\n",
      "Iteration 1057, loss = 0.00325306\n",
      "Iteration 1058, loss = 0.00324333\n",
      "Iteration 1059, loss = 0.00323380\n",
      "Iteration 1060, loss = 0.00322415\n",
      "Iteration 1061, loss = 0.00321447\n",
      "Iteration 1062, loss = 0.00320474\n",
      "Iteration 1063, loss = 0.00319531\n",
      "Iteration 1064, loss = 0.00318597\n",
      "Iteration 1065, loss = 0.00317686\n",
      "Iteration 1066, loss = 0.00316800\n",
      "Iteration 1067, loss = 0.00315920\n",
      "Iteration 1068, loss = 0.00315057\n",
      "Iteration 1069, loss = 0.00314200\n",
      "Iteration 1070, loss = 0.00313336\n",
      "Iteration 1071, loss = 0.00312474\n",
      "Iteration 1072, loss = 0.00311639\n",
      "Iteration 1073, loss = 0.00311270\n",
      "Iteration 1074, loss = 0.00311443\n",
      "Iteration 1075, loss = 0.00312896\n",
      "Iteration 1076, loss = 0.00315261\n",
      "Iteration 1077, loss = 0.00314577\n",
      "Iteration 1078, loss = 0.00311927\n",
      "Iteration 1079, loss = 0.00307364\n",
      "Iteration 1080, loss = 0.00304941\n",
      "Iteration 1081, loss = 0.00305335\n",
      "Iteration 1082, loss = 0.00306227\n",
      "Iteration 1083, loss = 0.00305610\n",
      "Iteration 1084, loss = 0.00302988\n",
      "Iteration 1085, loss = 0.00300874\n",
      "Iteration 1086, loss = 0.00300254\n",
      "Iteration 1087, loss = 0.00300941\n",
      "Iteration 1088, loss = 0.00302057\n",
      "Iteration 1089, loss = 0.00301254\n",
      "Iteration 1090, loss = 0.00298756\n",
      "Iteration 1091, loss = 0.00296171\n",
      "Iteration 1092, loss = 0.00295656\n",
      "Iteration 1093, loss = 0.00296332\n",
      "Iteration 1094, loss = 0.00295822\n",
      "Iteration 1095, loss = 0.00294045\n",
      "Iteration 1096, loss = 0.00292219\n",
      "Iteration 1097, loss = 0.00291683\n",
      "Iteration 1098, loss = 0.00291804\n",
      "Iteration 1099, loss = 0.00291134\n",
      "Iteration 1100, loss = 0.00289698\n",
      "Iteration 1101, loss = 0.00288350\n",
      "Iteration 1102, loss = 0.00287789\n",
      "Iteration 1103, loss = 0.00287559\n",
      "Iteration 1104, loss = 0.00286818\n",
      "Iteration 1105, loss = 0.00285631\n",
      "Iteration 1106, loss = 0.00284591\n",
      "Iteration 1107, loss = 0.00284026\n",
      "Iteration 1108, loss = 0.00283573\n",
      "Iteration 1109, loss = 0.00282783\n",
      "Iteration 1110, loss = 0.00281771\n",
      "Iteration 1111, loss = 0.00280919\n",
      "Iteration 1112, loss = 0.00280339\n",
      "Iteration 1113, loss = 0.00279757\n",
      "Iteration 1114, loss = 0.00278956\n",
      "Iteration 1115, loss = 0.00278063\n",
      "Iteration 1116, loss = 0.00277294\n",
      "Iteration 1117, loss = 0.00276617\n",
      "Iteration 1118, loss = 0.00275937\n",
      "Iteration 1119, loss = 0.00275187\n",
      "Iteration 1120, loss = 0.00274428\n",
      "Iteration 1121, loss = 0.00273719\n",
      "Iteration 1122, loss = 0.00273113\n",
      "Iteration 1123, loss = 0.00272588\n",
      "Iteration 1124, loss = 0.00271933\n",
      "Iteration 1125, loss = 0.00271077\n",
      "Iteration 1126, loss = 0.00270258\n",
      "Iteration 1127, loss = 0.00269608\n",
      "Iteration 1128, loss = 0.00268952\n",
      "Iteration 1129, loss = 0.00268731\n",
      "Iteration 1130, loss = 0.00269022\n",
      "Iteration 1131, loss = 0.00269027\n",
      "Iteration 1132, loss = 0.00267438\n",
      "Iteration 1133, loss = 0.00265586\n",
      "Iteration 1134, loss = 0.00265163\n",
      "Iteration 1135, loss = 0.00265335\n",
      "Iteration 1136, loss = 0.00265018\n",
      "Iteration 1137, loss = 0.00263922\n",
      "Iteration 1138, loss = 0.00262461\n",
      "Iteration 1139, loss = 0.00261628\n",
      "Iteration 1140, loss = 0.00261718\n",
      "Iteration 1141, loss = 0.00261745\n",
      "Iteration 1142, loss = 0.00260549\n",
      "Iteration 1143, loss = 0.00259010\n",
      "Iteration 1144, loss = 0.00258500\n",
      "Iteration 1145, loss = 0.00258526\n",
      "Iteration 1146, loss = 0.00257812\n",
      "Iteration 1147, loss = 0.00256563\n",
      "Iteration 1148, loss = 0.00255727\n",
      "Iteration 1149, loss = 0.00255472\n",
      "Iteration 1150, loss = 0.00255023\n",
      "Iteration 1151, loss = 0.00254035\n",
      "Iteration 1152, loss = 0.00253121\n",
      "Iteration 1153, loss = 0.00252622\n",
      "Iteration 1154, loss = 0.00252166\n",
      "Iteration 1155, loss = 0.00251360\n",
      "Iteration 1156, loss = 0.00250659\n",
      "Iteration 1157, loss = 0.00249950\n",
      "Iteration 1158, loss = 0.00249252\n",
      "Iteration 1159, loss = 0.00248664\n",
      "Iteration 1160, loss = 0.00248097\n",
      "Iteration 1161, loss = 0.00247473\n",
      "Iteration 1162, loss = 0.00246816\n",
      "Iteration 1163, loss = 0.00246179\n",
      "Iteration 1164, loss = 0.00245585\n",
      "Iteration 1165, loss = 0.00244999\n",
      "Iteration 1166, loss = 0.00244409\n",
      "Iteration 1167, loss = 0.00243784\n",
      "Iteration 1168, loss = 0.00243169\n",
      "Iteration 1169, loss = 0.00242543\n",
      "Iteration 1170, loss = 0.00241923\n",
      "Iteration 1171, loss = 0.00241321\n",
      "Iteration 1172, loss = 0.00240725\n",
      "Iteration 1173, loss = 0.00240163\n",
      "Iteration 1174, loss = 0.00239610\n",
      "Iteration 1175, loss = 0.00239054\n",
      "Iteration 1176, loss = 0.00238514\n",
      "Iteration 1177, loss = 0.00237897\n",
      "Iteration 1178, loss = 0.00237201\n",
      "Iteration 1179, loss = 0.00236529\n",
      "Iteration 1180, loss = 0.00235968\n",
      "Iteration 1181, loss = 0.00235529\n",
      "Iteration 1182, loss = 0.00235245\n",
      "Iteration 1183, loss = 0.00235361\n",
      "Iteration 1184, loss = 0.00235037\n",
      "Iteration 1185, loss = 0.00233783\n",
      "Iteration 1186, loss = 0.00232563\n",
      "Iteration 1187, loss = 0.00232288\n",
      "Iteration 1188, loss = 0.00232307\n",
      "Iteration 1189, loss = 0.00232399\n",
      "Iteration 1190, loss = 0.00233163\n",
      "Iteration 1191, loss = 0.00232140\n",
      "Iteration 1192, loss = 0.00229695\n",
      "Iteration 1193, loss = 0.00228883\n",
      "Iteration 1194, loss = 0.00229641\n",
      "Iteration 1195, loss = 0.00229214\n",
      "Iteration 1196, loss = 0.00227286\n",
      "Iteration 1197, loss = 0.00226372\n",
      "Iteration 1198, loss = 0.00226802\n",
      "Iteration 1199, loss = 0.00227388\n",
      "Iteration 1200, loss = 0.00227870\n",
      "Iteration 1201, loss = 0.00226091\n",
      "Iteration 1202, loss = 0.00223772\n",
      "Iteration 1203, loss = 0.00223778\n",
      "Iteration 1204, loss = 0.00224373\n",
      "Iteration 1205, loss = 0.00223175\n",
      "Iteration 1206, loss = 0.00221487\n",
      "Iteration 1207, loss = 0.00221476\n",
      "Iteration 1208, loss = 0.00221764\n",
      "Iteration 1209, loss = 0.00220900\n",
      "Iteration 1210, loss = 0.00219630\n",
      "Iteration 1211, loss = 0.00218752\n",
      "Iteration 1212, loss = 0.00218589\n",
      "Iteration 1213, loss = 0.00218369\n",
      "Iteration 1214, loss = 0.00217513\n",
      "Iteration 1215, loss = 0.00216690\n",
      "Iteration 1216, loss = 0.00216365\n",
      "Iteration 1217, loss = 0.00216050\n",
      "Iteration 1218, loss = 0.00215369\n",
      "Iteration 1219, loss = 0.00214609\n",
      "Iteration 1220, loss = 0.00214126\n",
      "Iteration 1221, loss = 0.00213813\n",
      "Iteration 1222, loss = 0.00213336\n",
      "Iteration 1223, loss = 0.00212664\n",
      "Iteration 1224, loss = 0.00212023\n",
      "Iteration 1225, loss = 0.00211588\n",
      "Iteration 1226, loss = 0.00211187\n",
      "Iteration 1227, loss = 0.00210620\n",
      "Iteration 1228, loss = 0.00210001\n",
      "Iteration 1229, loss = 0.00209497\n",
      "Iteration 1230, loss = 0.00209080\n",
      "Iteration 1231, loss = 0.00208623\n",
      "Iteration 1232, loss = 0.00208063\n",
      "Iteration 1233, loss = 0.00207491\n",
      "Iteration 1234, loss = 0.00206991\n",
      "Iteration 1235, loss = 0.00206544\n",
      "Iteration 1236, loss = 0.00206067\n",
      "Iteration 1237, loss = 0.00205532\n",
      "Iteration 1238, loss = 0.00205004\n",
      "Iteration 1239, loss = 0.00204520\n",
      "Iteration 1240, loss = 0.00204061\n",
      "Iteration 1241, loss = 0.00203585\n",
      "Iteration 1242, loss = 0.00203076\n",
      "Iteration 1243, loss = 0.00202562\n",
      "Iteration 1244, loss = 0.00202071\n",
      "Iteration 1245, loss = 0.00201602\n",
      "Iteration 1246, loss = 0.00201131\n",
      "Iteration 1247, loss = 0.00200643\n",
      "Iteration 1248, loss = 0.00200147\n",
      "Iteration 1249, loss = 0.00199659\n",
      "Iteration 1250, loss = 0.00199183\n",
      "Iteration 1251, loss = 0.00198714\n",
      "Iteration 1252, loss = 0.00198243\n",
      "Iteration 1253, loss = 0.00197764\n",
      "Iteration 1254, loss = 0.00197282\n",
      "Iteration 1255, loss = 0.00196804\n",
      "Iteration 1256, loss = 0.00196334\n",
      "Iteration 1257, loss = 0.00195870\n",
      "Iteration 1258, loss = 0.00195405\n",
      "Iteration 1259, loss = 0.00194937\n",
      "Iteration 1260, loss = 0.00194466\n",
      "Iteration 1261, loss = 0.00193996\n",
      "Iteration 1262, loss = 0.00193531\n",
      "Iteration 1263, loss = 0.00193069\n",
      "Iteration 1264, loss = 0.00192610\n",
      "Iteration 1265, loss = 0.00192152\n",
      "Iteration 1266, loss = 0.00191693\n",
      "Iteration 1267, loss = 0.00191235\n",
      "Iteration 1268, loss = 0.00190776\n",
      "Iteration 1269, loss = 0.00190319\n",
      "Iteration 1270, loss = 0.00189864\n",
      "Iteration 1271, loss = 0.00189411\n",
      "Iteration 1272, loss = 0.00188960\n",
      "Iteration 1273, loss = 0.00188511\n",
      "Iteration 1274, loss = 0.00188063\n",
      "Iteration 1275, loss = 0.00187616\n",
      "Iteration 1276, loss = 0.00187171\n",
      "Iteration 1277, loss = 0.00186726\n",
      "Iteration 1278, loss = 0.00186282\n",
      "Iteration 1279, loss = 0.00185840\n",
      "Iteration 1280, loss = 0.00185398\n",
      "Iteration 1281, loss = 0.00184958\n",
      "Iteration 1282, loss = 0.00184520\n",
      "Iteration 1283, loss = 0.00184082\n",
      "Iteration 1284, loss = 0.00183647\n",
      "Iteration 1285, loss = 0.00183214\n",
      "Iteration 1286, loss = 0.00182783\n",
      "Iteration 1287, loss = 0.00182355\n",
      "Iteration 1288, loss = 0.00181932\n",
      "Iteration 1289, loss = 0.00181516\n",
      "Iteration 1290, loss = 0.00181114\n",
      "Iteration 1291, loss = 0.00180729\n",
      "Iteration 1292, loss = 0.00180395\n",
      "Iteration 1293, loss = 0.00180095\n",
      "Iteration 1294, loss = 0.00179933\n",
      "Iteration 1295, loss = 0.00179688\n",
      "Iteration 1296, loss = 0.00179537\n",
      "Iteration 1297, loss = 0.00178881\n",
      "Iteration 1298, loss = 0.00178142\n",
      "Iteration 1299, loss = 0.00177361\n",
      "Iteration 1300, loss = 0.00176874\n",
      "Iteration 1301, loss = 0.00176656\n",
      "Iteration 1302, loss = 0.00176524\n",
      "Iteration 1303, loss = 0.00176408\n",
      "Iteration 1304, loss = 0.00175890\n",
      "Iteration 1305, loss = 0.00175206\n",
      "Iteration 1306, loss = 0.00174488\n",
      "Iteration 1307, loss = 0.00174063\n",
      "Iteration 1308, loss = 0.00173878\n",
      "Iteration 1309, loss = 0.00173686\n",
      "Iteration 1310, loss = 0.00173395\n",
      "Iteration 1311, loss = 0.00172788\n",
      "Iteration 1312, loss = 0.00172148\n",
      "Iteration 1313, loss = 0.00171628\n",
      "Iteration 1314, loss = 0.00171465\n",
      "Iteration 1315, loss = 0.00171651\n",
      "Iteration 1316, loss = 0.00171785\n",
      "Iteration 1317, loss = 0.00171372\n",
      "Iteration 1318, loss = 0.00170094\n",
      "Iteration 1319, loss = 0.00169339\n",
      "Iteration 1320, loss = 0.00169440\n",
      "Iteration 1321, loss = 0.00169503\n",
      "Iteration 1322, loss = 0.00169056\n",
      "Iteration 1323, loss = 0.00168062\n",
      "Iteration 1324, loss = 0.00167459\n",
      "Iteration 1325, loss = 0.00167415\n",
      "Iteration 1326, loss = 0.00167285\n",
      "Iteration 1327, loss = 0.00166774\n",
      "Iteration 1328, loss = 0.00166008\n",
      "Iteration 1329, loss = 0.00165594\n",
      "Iteration 1330, loss = 0.00165459\n",
      "Iteration 1331, loss = 0.00165207\n",
      "Iteration 1332, loss = 0.00164775\n",
      "Iteration 1333, loss = 0.00164176\n",
      "Iteration 1334, loss = 0.00163725\n",
      "Iteration 1335, loss = 0.00163502\n",
      "Iteration 1336, loss = 0.00163222\n",
      "Iteration 1337, loss = 0.00162772\n",
      "Iteration 1338, loss = 0.00162297\n",
      "Iteration 1339, loss = 0.00161953\n",
      "Iteration 1340, loss = 0.00161685\n",
      "Iteration 1341, loss = 0.00161345\n",
      "Iteration 1342, loss = 0.00160915\n",
      "Iteration 1343, loss = 0.00160503\n",
      "Iteration 1344, loss = 0.00160187\n",
      "Iteration 1345, loss = 0.00159891\n",
      "Iteration 1346, loss = 0.00159526\n",
      "Iteration 1347, loss = 0.00159125\n",
      "Iteration 1348, loss = 0.00158752\n",
      "Iteration 1349, loss = 0.00158430\n",
      "Iteration 1350, loss = 0.00158113\n",
      "Iteration 1351, loss = 0.00157754\n",
      "Iteration 1352, loss = 0.00157370\n",
      "Iteration 1353, loss = 0.00157005\n",
      "Iteration 1354, loss = 0.00156661\n",
      "Iteration 1355, loss = 0.00156318\n",
      "Iteration 1356, loss = 0.00156057\n",
      "Iteration 1357, loss = 0.00155779\n",
      "Iteration 1358, loss = 0.00155626\n",
      "Iteration 1359, loss = 0.00155272\n",
      "Iteration 1360, loss = 0.00154750\n",
      "Iteration 1361, loss = 0.00154318\n",
      "Iteration 1362, loss = 0.00154086\n",
      "Iteration 1363, loss = 0.00153880\n",
      "Iteration 1364, loss = 0.00153532\n",
      "Iteration 1365, loss = 0.00153063\n",
      "Iteration 1366, loss = 0.00152656\n",
      "Iteration 1367, loss = 0.00152356\n",
      "Iteration 1368, loss = 0.00152086\n",
      "Iteration 1369, loss = 0.00151725\n",
      "Iteration 1370, loss = 0.00151413\n",
      "Iteration 1371, loss = 0.00151129\n",
      "Iteration 1372, loss = 0.00150811\n",
      "Iteration 1373, loss = 0.00150417\n",
      "Iteration 1374, loss = 0.00149994\n",
      "Iteration 1375, loss = 0.00149644\n",
      "Iteration 1376, loss = 0.00149366\n",
      "Iteration 1377, loss = 0.00149125\n",
      "Iteration 1378, loss = 0.00148914\n",
      "Iteration 1379, loss = 0.00148626\n",
      "Iteration 1380, loss = 0.00148319\n",
      "Iteration 1381, loss = 0.00147867\n",
      "Iteration 1382, loss = 0.00147405\n",
      "Iteration 1383, loss = 0.00147049\n",
      "Iteration 1384, loss = 0.00146831\n",
      "Iteration 1385, loss = 0.00146733\n",
      "Iteration 1386, loss = 0.00146660\n",
      "Iteration 1387, loss = 0.00146809\n",
      "Iteration 1388, loss = 0.00146389\n",
      "Iteration 1389, loss = 0.00145530\n",
      "Iteration 1390, loss = 0.00144913\n",
      "Iteration 1391, loss = 0.00144925\n",
      "Iteration 1392, loss = 0.00144950\n",
      "Iteration 1393, loss = 0.00144618\n",
      "Iteration 1394, loss = 0.00144335\n",
      "Iteration 1395, loss = 0.00143650\n",
      "Iteration 1396, loss = 0.00143015\n",
      "Iteration 1397, loss = 0.00142705\n",
      "Iteration 1398, loss = 0.00142617\n",
      "Iteration 1399, loss = 0.00142459\n",
      "Iteration 1400, loss = 0.00142030\n",
      "Iteration 1401, loss = 0.00141568\n",
      "Iteration 1402, loss = 0.00141126\n",
      "Iteration 1403, loss = 0.00140795\n",
      "Iteration 1404, loss = 0.00140552\n",
      "Iteration 1405, loss = 0.00140341\n",
      "Iteration 1406, loss = 0.00140133\n",
      "Iteration 1407, loss = 0.00139808\n",
      "Iteration 1408, loss = 0.00139452\n",
      "Iteration 1409, loss = 0.00139051\n",
      "Iteration 1410, loss = 0.00138684\n",
      "Iteration 1411, loss = 0.00138357\n",
      "Iteration 1412, loss = 0.00138069\n",
      "Iteration 1413, loss = 0.00137851\n",
      "Iteration 1414, loss = 0.00137736\n",
      "Iteration 1415, loss = 0.00137977\n",
      "Iteration 1416, loss = 0.00138065\n",
      "Iteration 1417, loss = 0.00137986\n",
      "Iteration 1418, loss = 0.00136858\n",
      "Iteration 1419, loss = 0.00136060\n",
      "Iteration 1420, loss = 0.00136105\n",
      "Iteration 1421, loss = 0.00136761\n",
      "Iteration 1422, loss = 0.00138149\n",
      "Iteration 1423, loss = 0.00137552\n",
      "Iteration 1424, loss = 0.00135856\n",
      "Iteration 1425, loss = 0.00134386\n",
      "Iteration 1426, loss = 0.00135175\n",
      "Iteration 1427, loss = 0.00135565\n",
      "Iteration 1428, loss = 0.00133897\n",
      "Iteration 1429, loss = 0.00133277\n",
      "Iteration 1430, loss = 0.00133994\n",
      "Iteration 1431, loss = 0.00134187\n",
      "Iteration 1432, loss = 0.00133761\n",
      "Iteration 1433, loss = 0.00132327\n",
      "Iteration 1434, loss = 0.00131896\n",
      "Iteration 1435, loss = 0.00132404\n",
      "Iteration 1436, loss = 0.00131973\n",
      "Iteration 1437, loss = 0.00131060\n",
      "Iteration 1438, loss = 0.00130745\n",
      "Iteration 1439, loss = 0.00130949\n",
      "Iteration 1440, loss = 0.00130817\n",
      "Iteration 1441, loss = 0.00130070\n",
      "Iteration 1442, loss = 0.00129564\n",
      "Iteration 1443, loss = 0.00129522\n",
      "Iteration 1444, loss = 0.00129450\n",
      "Iteration 1445, loss = 0.00129069\n",
      "Iteration 1446, loss = 0.00128528\n",
      "Iteration 1447, loss = 0.00128281\n",
      "Iteration 1448, loss = 0.00128223\n",
      "Iteration 1449, loss = 0.00127965\n",
      "Iteration 1450, loss = 0.00127541\n",
      "Iteration 1451, loss = 0.00127164\n",
      "Iteration 1452, loss = 0.00126972\n",
      "Iteration 1453, loss = 0.00126825\n",
      "Iteration 1454, loss = 0.00126537\n",
      "Iteration 1455, loss = 0.00126172\n",
      "Iteration 1456, loss = 0.00125846\n",
      "Iteration 1457, loss = 0.00125624\n",
      "Iteration 1458, loss = 0.00125436\n",
      "Iteration 1459, loss = 0.00125185\n",
      "Iteration 1460, loss = 0.00124880\n",
      "Iteration 1461, loss = 0.00124559\n",
      "Iteration 1462, loss = 0.00124287\n",
      "Iteration 1463, loss = 0.00124061\n",
      "Iteration 1464, loss = 0.00123838\n",
      "Iteration 1465, loss = 0.00123588\n",
      "Iteration 1466, loss = 0.00123305\n",
      "Iteration 1467, loss = 0.00123018\n",
      "Iteration 1468, loss = 0.00122744\n",
      "Iteration 1469, loss = 0.00122490\n",
      "Iteration 1470, loss = 0.00122251\n",
      "Iteration 1471, loss = 0.00122015\n",
      "Iteration 1472, loss = 0.00121774\n",
      "Iteration 1473, loss = 0.00121520\n",
      "Iteration 1474, loss = 0.00121259\n",
      "Iteration 1475, loss = 0.00120993\n",
      "Iteration 1476, loss = 0.00120730\n",
      "Iteration 1477, loss = 0.00120472\n",
      "Iteration 1478, loss = 0.00120218\n",
      "Iteration 1479, loss = 0.00119969\n",
      "Iteration 1480, loss = 0.00119723\n",
      "Iteration 1481, loss = 0.00119480\n",
      "Iteration 1482, loss = 0.00119240\n",
      "Iteration 1483, loss = 0.00119005\n",
      "Iteration 1484, loss = 0.00118776\n",
      "Iteration 1485, loss = 0.00118563\n",
      "Iteration 1486, loss = 0.00118363\n",
      "Iteration 1487, loss = 0.00118213\n",
      "Iteration 1488, loss = 0.00118089\n",
      "Iteration 1489, loss = 0.00118095\n",
      "Iteration 1490, loss = 0.00118019\n",
      "Iteration 1491, loss = 0.00118036\n",
      "Iteration 1492, loss = 0.00117563\n",
      "Iteration 1493, loss = 0.00117007\n",
      "Iteration 1494, loss = 0.00116411\n",
      "Iteration 1495, loss = 0.00116132\n",
      "Iteration 1496, loss = 0.00116131\n",
      "Iteration 1497, loss = 0.00116184\n",
      "Iteration 1498, loss = 0.00116273\n",
      "Iteration 1499, loss = 0.00115981\n",
      "Iteration 1500, loss = 0.00115574\n",
      "Iteration 1501, loss = 0.00114928\n",
      "Iteration 1502, loss = 0.00114495\n",
      "Iteration 1503, loss = 0.00114371\n",
      "Iteration 1504, loss = 0.00114363\n",
      "Iteration 1505, loss = 0.00114270\n",
      "Iteration 1506, loss = 0.00113905\n",
      "Iteration 1507, loss = 0.00113489\n",
      "Iteration 1508, loss = 0.00113140\n",
      "Iteration 1509, loss = 0.00112918\n",
      "Iteration 1510, loss = 0.00112785\n",
      "Iteration 1511, loss = 0.00112637\n",
      "Iteration 1512, loss = 0.00112432\n",
      "Iteration 1513, loss = 0.00112138\n",
      "Iteration 1514, loss = 0.00111838\n",
      "Iteration 1515, loss = 0.00111578\n",
      "Iteration 1516, loss = 0.00111379\n",
      "Iteration 1517, loss = 0.00111207\n",
      "Iteration 1518, loss = 0.00111026\n",
      "Iteration 1519, loss = 0.00110831\n",
      "Iteration 1520, loss = 0.00110595\n",
      "Iteration 1521, loss = 0.00110351\n",
      "Iteration 1522, loss = 0.00110094\n",
      "Iteration 1523, loss = 0.00109848\n",
      "Iteration 1524, loss = 0.00109617\n",
      "Iteration 1525, loss = 0.00109399\n",
      "Iteration 1526, loss = 0.00109192\n",
      "Iteration 1527, loss = 0.00108991\n",
      "Iteration 1528, loss = 0.00108797\n",
      "Iteration 1529, loss = 0.00108603\n",
      "Iteration 1530, loss = 0.00108411\n",
      "Iteration 1531, loss = 0.00108209\n",
      "Iteration 1532, loss = 0.00108006\n",
      "Iteration 1533, loss = 0.00107787\n",
      "Iteration 1534, loss = 0.00107564\n",
      "Iteration 1535, loss = 0.00107335\n",
      "Iteration 1536, loss = 0.00107109\n",
      "Iteration 1537, loss = 0.00106887\n",
      "Iteration 1538, loss = 0.00106671\n",
      "Iteration 1539, loss = 0.00106460\n",
      "Iteration 1540, loss = 0.00106254\n",
      "Iteration 1541, loss = 0.00106051\n",
      "Iteration 1542, loss = 0.00105852\n",
      "Iteration 1543, loss = 0.00105655\n",
      "Iteration 1544, loss = 0.00105460\n",
      "Iteration 1545, loss = 0.00105269\n",
      "Iteration 1546, loss = 0.00105080\n",
      "Iteration 1547, loss = 0.00104899\n",
      "Iteration 1548, loss = 0.00104721\n",
      "Iteration 1549, loss = 0.00104557\n",
      "Iteration 1550, loss = 0.00104398\n",
      "Iteration 1551, loss = 0.00104265\n",
      "Iteration 1552, loss = 0.00104123\n",
      "Iteration 1553, loss = 0.00104014\n",
      "Iteration 1554, loss = 0.00103850\n",
      "Iteration 1555, loss = 0.00103699\n",
      "Iteration 1556, loss = 0.00103437\n",
      "Iteration 1557, loss = 0.00103166\n",
      "Iteration 1558, loss = 0.00102839\n",
      "Iteration 1559, loss = 0.00102547\n",
      "Iteration 1560, loss = 0.00102304\n",
      "Iteration 1561, loss = 0.00102119\n",
      "Iteration 1562, loss = 0.00101976\n",
      "Iteration 1563, loss = 0.00101850\n",
      "Iteration 1564, loss = 0.00101732\n",
      "Iteration 1565, loss = 0.00101583\n",
      "Iteration 1566, loss = 0.00101422\n",
      "Iteration 1567, loss = 0.00101200\n",
      "Iteration 1568, loss = 0.00100963\n",
      "Iteration 1569, loss = 0.00100700\n",
      "Iteration 1570, loss = 0.00100456\n",
      "Iteration 1571, loss = 0.00100242\n",
      "Iteration 1572, loss = 0.00100062\n",
      "Iteration 1573, loss = 0.00099906\n",
      "Iteration 1574, loss = 0.00099759\n",
      "Iteration 1575, loss = 0.00099613\n",
      "Iteration 1576, loss = 0.00099449\n",
      "Iteration 1577, loss = 0.00099277\n",
      "Iteration 1578, loss = 0.00099077\n",
      "Iteration 1579, loss = 0.00098872\n",
      "Iteration 1580, loss = 0.00098657\n",
      "Iteration 1581, loss = 0.00098450\n",
      "Iteration 1582, loss = 0.00098255\n",
      "Iteration 1583, loss = 0.00098073\n",
      "Iteration 1584, loss = 0.00097903\n",
      "Iteration 1585, loss = 0.00097740\n",
      "Iteration 1586, loss = 0.00097580\n",
      "Iteration 1587, loss = 0.00097419\n",
      "Iteration 1588, loss = 0.00097257\n",
      "Iteration 1589, loss = 0.00097088\n",
      "Iteration 1590, loss = 0.00096916\n",
      "Iteration 1591, loss = 0.00096735\n",
      "Iteration 1592, loss = 0.00096552\n",
      "Iteration 1593, loss = 0.00096365\n",
      "Iteration 1594, loss = 0.00096179\n",
      "Iteration 1595, loss = 0.00096000\n",
      "Iteration 1596, loss = 0.00095823\n",
      "Iteration 1597, loss = 0.00095645\n",
      "Iteration 1598, loss = 0.00095468\n",
      "Iteration 1599, loss = 0.00095296\n",
      "Iteration 1600, loss = 0.00095126\n",
      "Iteration 1601, loss = 0.00094956\n",
      "Iteration 1602, loss = 0.00094786\n",
      "Iteration 1603, loss = 0.00094616\n",
      "Iteration 1604, loss = 0.00094445\n",
      "Iteration 1605, loss = 0.00094277\n",
      "Iteration 1606, loss = 0.00094112\n",
      "Iteration 1607, loss = 0.00093945\n",
      "Iteration 1608, loss = 0.00093779\n",
      "Iteration 1609, loss = 0.00093616\n",
      "Iteration 1610, loss = 0.00093456\n",
      "Iteration 1611, loss = 0.00093299\n",
      "Iteration 1612, loss = 0.00093146\n",
      "Iteration 1613, loss = 0.00093003\n",
      "Iteration 1614, loss = 0.00092884\n",
      "Iteration 1615, loss = 0.00092795\n",
      "Iteration 1616, loss = 0.00092786\n",
      "Iteration 1617, loss = 0.00092819\n",
      "Iteration 1618, loss = 0.00092978\n",
      "Iteration 1619, loss = 0.00092973\n",
      "Iteration 1620, loss = 0.00092970\n",
      "Iteration 1621, loss = 0.00092449\n",
      "Iteration 1622, loss = 0.00091864\n",
      "Iteration 1623, loss = 0.00091358\n",
      "Iteration 1624, loss = 0.00091215\n",
      "Iteration 1625, loss = 0.00091336\n",
      "Iteration 1626, loss = 0.00091418\n",
      "Iteration 1627, loss = 0.00091372\n",
      "Iteration 1628, loss = 0.00090966\n",
      "Iteration 1629, loss = 0.00090495\n",
      "Iteration 1630, loss = 0.00090202\n",
      "Iteration 1631, loss = 0.00090179\n",
      "Iteration 1632, loss = 0.00090239\n",
      "Iteration 1633, loss = 0.00090113\n",
      "Iteration 1634, loss = 0.00089833\n",
      "Iteration 1635, loss = 0.00089483\n",
      "Iteration 1636, loss = 0.00089259\n",
      "Iteration 1637, loss = 0.00089186\n",
      "Iteration 1638, loss = 0.00089131\n",
      "Iteration 1639, loss = 0.00088990\n",
      "Iteration 1640, loss = 0.00088727\n",
      "Iteration 1641, loss = 0.00088473\n",
      "Iteration 1642, loss = 0.00088319\n",
      "Iteration 1643, loss = 0.00088230\n",
      "Iteration 1644, loss = 0.00088111\n",
      "Iteration 1645, loss = 0.00087919\n",
      "Iteration 1646, loss = 0.00087705\n",
      "Iteration 1647, loss = 0.00087531\n",
      "Iteration 1648, loss = 0.00087403\n",
      "Iteration 1649, loss = 0.00087282\n",
      "Iteration 1650, loss = 0.00087125\n",
      "Iteration 1651, loss = 0.00086938\n",
      "Iteration 1652, loss = 0.00086760\n",
      "Iteration 1653, loss = 0.00086611\n",
      "Iteration 1654, loss = 0.00086479\n",
      "Iteration 1655, loss = 0.00086337\n",
      "Iteration 1656, loss = 0.00086173\n",
      "Iteration 1657, loss = 0.00086001\n",
      "Iteration 1658, loss = 0.00085839\n",
      "Iteration 1659, loss = 0.00085696\n",
      "Iteration 1660, loss = 0.00085555\n",
      "Iteration 1661, loss = 0.00085405\n",
      "Iteration 1662, loss = 0.00085244\n",
      "Iteration 1663, loss = 0.00085083\n",
      "Iteration 1664, loss = 0.00084930\n",
      "Iteration 1665, loss = 0.00084783\n",
      "Iteration 1666, loss = 0.00084639\n",
      "Iteration 1667, loss = 0.00084488\n",
      "Iteration 1668, loss = 0.00084333\n",
      "Iteration 1669, loss = 0.00084177\n",
      "Iteration 1670, loss = 0.00084026\n",
      "Iteration 1671, loss = 0.00083879\n",
      "Iteration 1672, loss = 0.00083732\n",
      "Iteration 1673, loss = 0.00083584\n",
      "Iteration 1674, loss = 0.00083432\n",
      "Iteration 1675, loss = 0.00083280\n",
      "Iteration 1676, loss = 0.00083129\n",
      "Iteration 1677, loss = 0.00082980\n",
      "Iteration 1678, loss = 0.00082833\n",
      "Iteration 1679, loss = 0.00082686\n",
      "Iteration 1680, loss = 0.00082538\n",
      "Iteration 1681, loss = 0.00082389\n",
      "Iteration 1682, loss = 0.00082239\n",
      "Iteration 1683, loss = 0.00082089\n",
      "Iteration 1684, loss = 0.00081940\n",
      "Iteration 1685, loss = 0.00081791\n",
      "Iteration 1686, loss = 0.00081643\n",
      "Iteration 1687, loss = 0.00081495\n",
      "Iteration 1688, loss = 0.00081347\n",
      "Iteration 1689, loss = 0.00081200\n",
      "Iteration 1690, loss = 0.00081055\n",
      "Iteration 1691, loss = 0.00080917\n",
      "Iteration 1692, loss = 0.00080786\n",
      "Iteration 1693, loss = 0.00080689\n",
      "Iteration 1694, loss = 0.00080561\n",
      "Iteration 1695, loss = 0.00080455\n",
      "Iteration 1696, loss = 0.00080300\n",
      "Iteration 1697, loss = 0.00080120\n",
      "Iteration 1698, loss = 0.00079922\n",
      "Iteration 1699, loss = 0.00079745\n",
      "Iteration 1700, loss = 0.00079582\n",
      "Iteration 1701, loss = 0.00079447\n",
      "Iteration 1702, loss = 0.00079329\n",
      "Iteration 1703, loss = 0.00079195\n",
      "Iteration 1704, loss = 0.00079058\n",
      "Iteration 1705, loss = 0.00078900\n",
      "Iteration 1706, loss = 0.00078735\n",
      "Iteration 1707, loss = 0.00078576\n",
      "Iteration 1708, loss = 0.00078427\n",
      "Iteration 1709, loss = 0.00078285\n",
      "Iteration 1710, loss = 0.00078151\n",
      "Iteration 1711, loss = 0.00078020\n",
      "Iteration 1712, loss = 0.00077875\n",
      "Iteration 1713, loss = 0.00077729\n",
      "Iteration 1714, loss = 0.00077580\n",
      "Iteration 1715, loss = 0.00077430\n",
      "Iteration 1716, loss = 0.00077284\n",
      "Iteration 1717, loss = 0.00077145\n",
      "Iteration 1718, loss = 0.00077009\n",
      "Iteration 1719, loss = 0.00076870\n",
      "Iteration 1720, loss = 0.00076730\n",
      "Iteration 1721, loss = 0.00076590\n",
      "Iteration 1722, loss = 0.00076451\n",
      "Iteration 1723, loss = 0.00076307\n",
      "Iteration 1724, loss = 0.00076164\n",
      "Iteration 1725, loss = 0.00076022\n",
      "Iteration 1726, loss = 0.00075884\n",
      "Iteration 1727, loss = 0.00075748\n",
      "Iteration 1728, loss = 0.00075611\n",
      "Iteration 1729, loss = 0.00075475\n",
      "Iteration 1730, loss = 0.00075338\n",
      "Iteration 1731, loss = 0.00075200\n",
      "Iteration 1732, loss = 0.00075062\n",
      "Iteration 1733, loss = 0.00074923\n",
      "Iteration 1734, loss = 0.00074785\n",
      "Iteration 1735, loss = 0.00074649\n",
      "Iteration 1736, loss = 0.00074513\n",
      "Iteration 1737, loss = 0.00074378\n",
      "Iteration 1738, loss = 0.00074243\n",
      "Iteration 1739, loss = 0.00074108\n",
      "Iteration 1740, loss = 0.00073973\n",
      "Iteration 1741, loss = 0.00073839\n",
      "Iteration 1742, loss = 0.00073706\n",
      "Iteration 1743, loss = 0.00073574\n",
      "Iteration 1744, loss = 0.00073448\n",
      "Iteration 1745, loss = 0.00073331\n",
      "Iteration 1746, loss = 0.00073237\n",
      "Iteration 1747, loss = 0.00073125\n",
      "Iteration 1748, loss = 0.00072964\n",
      "Iteration 1749, loss = 0.00072782\n",
      "Iteration 1750, loss = 0.00072683\n",
      "Iteration 1751, loss = 0.00072577\n",
      "Iteration 1752, loss = 0.00072396\n",
      "Iteration 1753, loss = 0.00072280\n",
      "Iteration 1754, loss = 0.00072181\n",
      "Iteration 1755, loss = 0.00072011\n",
      "Iteration 1756, loss = 0.00071891\n",
      "Iteration 1757, loss = 0.00071783\n",
      "Iteration 1758, loss = 0.00071622\n",
      "Iteration 1759, loss = 0.00071506\n",
      "Iteration 1760, loss = 0.00071391\n",
      "Iteration 1761, loss = 0.00071237\n",
      "Iteration 1762, loss = 0.00071125\n",
      "Iteration 1763, loss = 0.00071005\n",
      "Iteration 1764, loss = 0.00070859\n",
      "Iteration 1765, loss = 0.00070745\n",
      "Iteration 1766, loss = 0.00070619\n",
      "Iteration 1767, loss = 0.00070476\n",
      "Iteration 1768, loss = 0.00070362\n",
      "Iteration 1769, loss = 0.00070238\n",
      "Iteration 1770, loss = 0.00070106\n",
      "Iteration 1771, loss = 0.00069988\n",
      "Iteration 1772, loss = 0.00069887\n",
      "Iteration 1773, loss = 0.00069816\n",
      "Iteration 1774, loss = 0.00069822\n",
      "Iteration 1775, loss = 0.00069781\n",
      "Iteration 1776, loss = 0.00069617\n",
      "Iteration 1777, loss = 0.00069413\n",
      "Iteration 1778, loss = 0.00069174\n",
      "Iteration 1779, loss = 0.00068983\n",
      "Iteration 1780, loss = 0.00068908\n",
      "Iteration 1781, loss = 0.00068773\n",
      "Iteration 1782, loss = 0.00068672\n",
      "Iteration 1783, loss = 0.00068626\n",
      "Iteration 1784, loss = 0.00068532\n",
      "Iteration 1785, loss = 0.00068387\n",
      "Iteration 1786, loss = 0.00068235\n",
      "Iteration 1787, loss = 0.00068073\n",
      "Iteration 1788, loss = 0.00067922\n",
      "Iteration 1789, loss = 0.00067788\n",
      "Iteration 1790, loss = 0.00067863\n",
      "Iteration 1791, loss = 0.00068477\n",
      "Iteration 1792, loss = 0.00069666\n",
      "Iteration 1793, loss = 0.00070217\n",
      "Iteration 1794, loss = 0.00068358\n",
      "Iteration 1795, loss = 0.00067162\n",
      "Iteration 1796, loss = 0.00067903\n",
      "Iteration 1797, loss = 0.00068271\n",
      "Iteration 1798, loss = 0.00067392\n",
      "Iteration 1799, loss = 0.00066736\n",
      "Iteration 1800, loss = 0.00067322\n",
      "Iteration 1801, loss = 0.00067464\n",
      "Iteration 1802, loss = 0.00066546\n",
      "Iteration 1803, loss = 0.00066406\n",
      "Iteration 1804, loss = 0.00066830\n",
      "Iteration 1805, loss = 0.00066418\n",
      "Iteration 1806, loss = 0.00065869\n",
      "Iteration 1807, loss = 0.00065991\n",
      "Iteration 1808, loss = 0.00066564\n",
      "Iteration 1809, loss = 0.00067172\n",
      "Iteration 1810, loss = 0.00066340\n",
      "Iteration 1811, loss = 0.00065372\n",
      "Iteration 1812, loss = 0.00065990\n",
      "Iteration 1813, loss = 0.00066013\n",
      "Iteration 1814, loss = 0.00065185\n",
      "Iteration 1815, loss = 0.00065228\n",
      "Iteration 1816, loss = 0.00065513\n",
      "Iteration 1817, loss = 0.00064986\n",
      "Iteration 1818, loss = 0.00064741\n",
      "Iteration 1819, loss = 0.00065011\n",
      "Iteration 1820, loss = 0.00064723\n",
      "Iteration 1821, loss = 0.00064367\n",
      "Iteration 1822, loss = 0.00064537\n",
      "Iteration 1823, loss = 0.00064387\n",
      "Iteration 1824, loss = 0.00064014\n",
      "Iteration 1825, loss = 0.00064077\n",
      "Iteration 1826, loss = 0.00064013\n",
      "Iteration 1827, loss = 0.00063722\n",
      "Iteration 1828, loss = 0.00063647\n",
      "Iteration 1829, loss = 0.00063720\n",
      "Iteration 1830, loss = 0.00063539\n",
      "Iteration 1831, loss = 0.00063276\n",
      "Iteration 1832, loss = 0.00063237\n",
      "Iteration 1833, loss = 0.00063207\n",
      "Iteration 1834, loss = 0.00063015\n",
      "Iteration 1835, loss = 0.00062901\n",
      "Iteration 1836, loss = 0.00062867\n",
      "Iteration 1837, loss = 0.00062718\n",
      "Iteration 1838, loss = 0.00062569\n",
      "Iteration 1839, loss = 0.00062525\n",
      "Iteration 1840, loss = 0.00062453\n",
      "Iteration 1841, loss = 0.00062294\n",
      "Iteration 1842, loss = 0.00062174\n",
      "Iteration 1843, loss = 0.00062113\n",
      "Iteration 1844, loss = 0.00062012\n",
      "Iteration 1845, loss = 0.00061880\n",
      "Iteration 1846, loss = 0.00061791\n",
      "Iteration 1847, loss = 0.00061710\n",
      "Iteration 1848, loss = 0.00061592\n",
      "Iteration 1849, loss = 0.00061480\n",
      "Iteration 1850, loss = 0.00061399\n",
      "Iteration 1851, loss = 0.00061311\n",
      "Iteration 1852, loss = 0.00061196\n",
      "Iteration 1853, loss = 0.00061087\n",
      "Iteration 1854, loss = 0.00061000\n",
      "Iteration 1855, loss = 0.00060909\n",
      "Iteration 1856, loss = 0.00060802\n",
      "Iteration 1857, loss = 0.00060699\n",
      "Iteration 1858, loss = 0.00060608\n",
      "Iteration 1859, loss = 0.00060516\n",
      "Iteration 1860, loss = 0.00060414\n",
      "Iteration 1861, loss = 0.00060310\n",
      "Iteration 1862, loss = 0.00060215\n",
      "Iteration 1863, loss = 0.00060123\n",
      "Iteration 1864, loss = 0.00060026\n",
      "Iteration 1865, loss = 0.00059925\n",
      "Iteration 1866, loss = 0.00059827\n",
      "Iteration 1867, loss = 0.00059733\n",
      "Iteration 1868, loss = 0.00059639\n",
      "Iteration 1869, loss = 0.00059542\n",
      "Iteration 1870, loss = 0.00059444\n",
      "Iteration 1871, loss = 0.00059345\n",
      "Iteration 1872, loss = 0.00059249\n",
      "Iteration 1873, loss = 0.00059155\n",
      "Iteration 1874, loss = 0.00059060\n",
      "Iteration 1875, loss = 0.00058964\n",
      "Iteration 1876, loss = 0.00058867\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.95      0.97        20\n",
      "           2       0.90      1.00      0.95        18\n",
      "           3       1.00      0.94      0.97        16\n",
      "\n",
      "    accuracy                           0.96        54\n",
      "   macro avg       0.97      0.96      0.96        54\n",
      "weighted avg       0.97      0.96      0.96        54\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# activation = ’relu’ # the default\n",
    "# 激發函數為預設的 relu 時，所需要的隱藏層的神經元要相對於 logistic 多些， 因為 relu 接近線性的函數對應，\n",
    "# 會降低整個網路的非線性關係，雖然可以避 免過度訓練，但它的近乎線性的對應關係，需要較多的神經元才能達到「學習」的目的\n",
    "hidden_layers = (512, ) # one hidden layer #output decide by model #30個神經元\n",
    "activation = \"logistic\"\n",
    "opts = dict(hidden_layer_sizes = hidden_layers, verbose = True, \\\n",
    "activation = activation, tol = 1e-6, max_iter = int(1e6))\n",
    "# solver = ’sgd’ # not efficient, need more tuning\n",
    "# solver = ’lbfgs’ # not suitable here\n",
    "solver = \"adam\" # default solver\n",
    "\n",
    "clf_MLP = MLPClassifier(solver = solver, **opts)\n",
    "clf_MLP.fit(X_train, y_train)\n",
    "predictions = clf_MLP.predict(X_test)\n",
    "\n",
    "#print((\"accuracy for test data : {:.2f}%\".format(100*np.mean(y_test_hat == y_test)))\n",
    "#print(\"accuracy for test data : {:.2f}%\".format(100*clf_MLP.score(X_test, y_test)))\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "plt.plot(clf_MLP.loss_curve_)\n",
    "plt.grid(True)\n",
    "plt.title(\"Loss Curve\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.figure(figsize = (8, 8))\n",
    "\n",
    "img_dir = \"/Users/guoyixuan/Documents/pythoncode/ccwML/\"\n",
    "plt.savefig(img_dir + \"MLP_loss_curve.png\", dpi = 300)\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (8, 8))\n",
    "#confusion matrix\n",
    "title = \"Testing score = {:.2f}%\".format(100*clf_MLP.score(X_test, y_test))\n",
    "disp = ConfusionMatrixDisplay.from_estimator(\n",
    "    clf_MLP, \n",
    "    X_test, \n",
    "    y_test, \n",
    "    xticks_rotation = \"vertical\",\n",
    "    #display labels\n",
    "    ax = ax, \n",
    "    cmap = plt.cm.Blues,\n",
    "    normalize = \"true\")\n",
    "\n",
    "disp.ax_.set_title(title)\n",
    "#plt.show()\n",
    "#plot_confusion_matrix(clf, X_test, y_test, cmap = plt.cm.Blues, normalize = \"true\")\n",
    "#plt.title(\"testing score = {:.2f}%\".format(100*clf.score(X_test, y_test)))\n",
    "plt.savefig(img_dir + \"neuro.eps\", format = \"eps\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccwmachine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
